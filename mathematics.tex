\chapter{Mathematical Theory}\label{chap:mathematics}
In this chapter a general introduction to multiwavelet theory will be given through 
the concept of multiresolution analysis (MRA)\footnote{Mallat uses the term 
multiresolution \emph{approximation}, but in this work we will use multiresolution 
\emph{analysis}, as it is more commonly used in the literature.}, that was developed 
by Mallat\cite{Mallat:1989} and Daubechies\cite{Daubechies:1988} in the late 1980s. 
A detailed description of MRAs can be found in Keinert\cite{Keinert:2004}, from 
which a brief summary of the key issues are given in the following, with the 
difference that we limit our discussion to the unit interval instead of the real line.

\section{Multiresolution Analysis}
A multiresolution analysis of $L^2([0,1])$ is an infinite nested sequence of subspaces
\begin{equation}
    \label{eq:MRA}
    \scalingspace{0}\subset \scalingspace{1}\subset \cdots \subset 
	\scalingspace{n}\subset \cdots \subset L^2([0,1])
\end{equation}
with the following properties
\begin{enumerate}
    \item $\bigcup_{n=0}^{\infty}\scalingspace{n}\ is\ dense\ in\ L^2([0,1])$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(2x) \in \scalingspace{n+1}
    	\ ,\ \forall n \in \mathbb{N}$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(x-2^{-n}l) \in 
	\scalingspace{n}\ ,\ \forall n \in \mathbb{N},\ 0 \leq l \leq 2^n - 1.$
    \item There exists a function vector $\scalingvec$ in $L^2([0,1])$ of length
	$k+1$ such that the vector components $\scaling_i$ forms a basis of 
	\scalingspace{0}. 
\end{enumerate}
This means that if we can construct a basis of \scalingspace{0}, which consists of 
only $k+1$ functions, we can construct a basis of \emph{any} space \scalingspace{n}, 
by simple compression (by a factor of $2^n$, property 2), and translations (to all 
dyadic grid points at scale $n$, property 3), of the original $k+1$ functions, and 
by increasing the scale $n$, we are approaching a complete basis of $L^2([0,1])$. 
Since $\scalingspace{n} \subset \scalingspace{n+1}$ the basis functions of 
\scalingspace{n} can be expanded in the basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalescaling}
    \scaling^n_{i,l}(x) \mydef 2^{n/2}\scaling_i(2^nx-l) = 
	\sum_{m=0}^{2^n-1} \sum_{j=0}^k H_{ij}^{(m)} \scaling^{n+1}_{j,m}(x)
\end{equation}
where the $H^{(m)}$s are the so-called filter matrices that describes the 
transformation between different spaces \scalingspace{n}. The MRA is called 
orthogonal if 
\begin{equation}
    \label{eq:orthogonality}
    \langle \scaling_{i,l}^n, \scaling_{j,m}^n \rangle = \delta_{i,j}\delta_{l,m}
\end{equation}
This orthogonality condition means that the 
functions are orthogonal both within one function vector and through all 
possible translations on one scale, but \emph{not} through the different scales.

Complementary to the nested sequence of subspaces \scalingspace{n}, we can define 
another series of spaces \waveletspace{n} that complements \scalingspace{n} in 
\scalingspace{n+1}
\begin{equation}
    \label{eq:MRAcomplement}
    \scalingspace{n+1} = \scalingspace{n} \oplus \waveletspace{n}
\end{equation}
where there exists another function vector $\waveletvec$ of lenght $k+1$ that, with 
all its translations on scale $n$ form a basis for \waveletspace{n}.
Analogously to Eq.~(\ref{eq:twoscalescaling}) the function vector can be expanded 
in the basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalewavelet}
    \wavelet^n_{i,l}(x) \mydef 2^{n/2}\wavelet_i(2^nx-l) = 
	\sum_{m=0}^{2^n-1} \sum_{j=0}^k G_{ij}^{(m)}\scaling_{j,m}^{n+1}(x)
\end{equation}
with filter matrices $G^{(m)}$. In orthogonal MRA the functions $\waveletvec$ fulfill 
the same othogonality condition as Eq.~(\ref{eq:orthogonality}), and if we combine 
Eq.~(\ref{eq:MRA}) and Eq.~(\ref{eq:MRAcomplement}) we see that they must also be 
orthogonal with respect to different scales
\begin{equation}
    \langle \wavelet_{j,l}^n, \wavelet_{i,m}^{n'} \rangle = 
	\delta_{i,j}\delta_{l,m}\delta_{n,n'}
\end{equation}
Recursive application of Eq.~(\ref{eq:MRAcomplement}) yields the important relation
\begin{equation}
    \label{eq:MRArecursive}
    \scalingspace{n} = \scalingspace{0} \oplus \waveletspace{0} \oplus \waveletspace{1}
	\oplus \cdots \oplus \waveletspace{n-1}
\end{equation}

\section{Multiwavelets}
There are many ways to choose the basis functions $\scalingvec$ and $\waveletvec$ 
(which define the spanned spaces \scalingspace{n} and \waveletspace{n},
leading to different wavelet families. There is a one-to-one correspondence between 
the basis functions $\scalingvec$ and $\waveletvec$, and the filter matrices $H^{(l)}$ 
and $G^{(l)}$ used in the two-scale relations Eq.~(\ref{eq:twoscalescaling}) and 
Eq.~(\ref{eq:twoscalewavelet}), and most well known wavelet families are defined 
only through their filter coefficients, such as Daubechies' family of compactly
supported wavelets\cite{Daubechies:1988}.

In the following we are taking a different approach, which follows the original 
construction of multiwavelets by Alpert\cite{Alpert:1993p5460}. We define the 
\emph{scaling space} \scalingspace{n} as the space of piecewise polynomials
\begin{equation}
    \label{eq:scaling_space_def}
    \begin{array}{rl}
        \displaystyle \scalingspace{n} \mydef & 
	    \lbrace f:\ all\ polynomials\ of\ degree\ \leq\ k\\ 
	\displaystyle &\ on\ the\ interval\ (2^{-n}l,2^{-n}(l+1))\\
	\displaystyle &\ for\ 0 \leq l < 2^n,\ f\ vanishes\ elsewhere \rbrace
    \end{array}
\end{equation}
This definition fulfills the conditions for a multiresolution analysis, and if the 
basis is chosen to be orthogonal, the \scalingspace{n} constitutes an \emph{orthogonal} MRA.

\subsection{The scaling basis}
The construction of the scaling functions is quite straightforward; $k+1$ orthogonal 
polynomials are chosen to span the space of polynomials of degree $\leq k$ on the unit 
interval. The total scaling basis for \scalingspace{n} is then obtained by appropriate dilation 
and translation of these functions. One way to construct the basis is to start with the 
standard basis $\lbrace 1,x,x^2,\dots,x^k\rbrace$ and orthonormalize with respect to 
the $L^2$ inner product on the unit interval.

\subsection{The wavelet basis}
The \emph{wavelet space} \waveletspace{n} is defined, according to 
Eq.~(\ref{eq:MRAcomplement}), as the orthogonal complement of \scalingspace{n} 
in \scalingspace{n+1}. The wavelet basis functions of \waveletspace{n} are hence 
piecewise polynomials of degree $\leq k$ on \emph{each} of the two intervals on 
scale $n+1$ that overlaps with \emph{one} interval on scale $n$ (but may be 
discontinous in the merging point). In the construction of the wavelet basis 
these piecewise polynomials should be made orthogonal both to the scaling basis 
of \scalingspace{n} and to each other.

One important property of the wavelet basis is its number of vanishing moments. 
The m-th continuous moment of a function $\wavelet$ is defined as the integral
\begin{equation}
    \mu_m \mydef \int_0^1 x^m \wavelet(x)dx 	
\end{equation}
and the function $\wavelet$ is said to have $M$ vanishing moments if 
\begin{equation}
    \mu_m = 0, \qquad m=0,\dots, M-1
\end{equation}
The vanishing moments of the wavelet functions gives information on the 
approximation order of the scaling functions. If the wavelet function $\wavelet$ 
has $M$ vanishing moments, any polynomial of order $\leq M-1$ can be exactly 
reproduced in the scaling space, and the error in representing an arbitrary 
function in the scaling basis is of $M$-th order. By construction, $x^m$ is in 
the space \scalingspace{0} for $0\leq m \leq k$, and since $\waveletspace{0} 
\perp \scalingspace{0}$, the first $k+1$ moments of $\wavelet^0_j$ must vanish.

\subsection{Filter relations}
With the multiwavelet basis defined, we can construct the filter matrices that 
fulfill the two-scale relations in Eq.(\ref{eq:twoscalescaling}) and
Eq.(\ref{eq:twoscalewavelet}). The exact construction will depend on the choice
of scaling and wavelet polynomials, and will not be treated here, but some 
important properties of the filter matrices are already apparent from the
definition of the scaling spaces given in Eq.~(\ref{eq:scaling_space_def}). 

Because of the disjoint support of the basis polynomials it is clear that
a basis vector at scale $n$ will overlap with two basis vectors at scale $n+1$,
and we end up with four matrices $H^{(0)}, H^{(1)}, G^{(0)}$ and $G^{(1)}$, 
each of size $(k+1) \times (k+1)$. Eq.~(\ref{eq:twoscalescaling}) and 
Eq.~(\ref{eq:twoscalewavelet}) thus reduces to

\begin{equation}
    \label{eq:twoscalerelations}
    \begin{pmatrix}
	\waveletvec_{l}^{n}\\
	\scalingvec_{l}^{n}\\
    \end{pmatrix}=
    \begin{pmatrix}
	G^{(1)}&G^{(0)}\\
	H^{(1)}&H^{(0)}\\
    \end{pmatrix}
    \begin{pmatrix}
	\scalingvec_{2l+1}^{n+1}\\
	\scalingvec_{2l}^{n+1}\\
    \end{pmatrix}
\end{equation}
\\
\noindent
The locality of this transformation is important for numerical implementations,
as it leads to efficient, linear scaling algorithms. The transformation in 
Eq.~(\ref{eq:twoscalerelations}) is called forward wavelet transform or 
wavelet decomposition, while its inverse is called backward wavelet transform
or wavelet reconstruction.

\subsection{Multiwavelets in $d$ dimensions}
Multi-dimensional wavelets are usually constructed by tensor products, where the
scaling space is defined as
\begin{equation}
    \scalingspace{n,d} \mydef \bigotimes^d \scalingspace{n}
\end{equation}
The basis for this $d$-dimensional space is given as tensor products of the
one-dimensional bases
\begin{equation}
    \label{eq:multidimscaling}
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x}) = 
    \scalingnd^n_{j_1 j_2\dots j_d,l_1 l_2\dots l_d} (x_1,x_2,\dots,x_d) \mydef
    \prod_{p=1}^d \scaling^n_{j_p,l_p}(x_p)
\end{equation}
The number of basis functions on each hypercube $\bs{l}=(l_1,l_2,\dots,l_d)$ 
becomes $(k+1)^d$, while the number of such hypercubes on scale $n$ becomes $2^{dn}$, 
which means that the total number of basis functions is growing exponentially 
with the number of dimensions.

The wavelet space can be defined using Eq.~(\ref{eq:MRAcomplement})
\begin{equation}
    \label{eq:multidimW}
    \scalingspace{n+1,d} = \bigotimes^d \scalingspace{n+1} = 
	\bigotimes^d (\scalingspace{n} \oplus \waveletspace{n})
\end{equation}
where the pure scaling term obtained when expanding the product on the right
hand side of Eq.~(\ref{eq:multidimW}) is recognized as \scalingspace{n,d}, making the
wavelet space \waveletspace{n,d} consist of all the remaining terms of the product, 
which are terms that contain at least one wavelet space.

To achieve a uniform notation, we can introduce a ``generalized'' one-dimensional
wavelet function $\lbrace\scalewave_{j,l}^{\alpha,n}\rbrace$ that, depending on 
the index $\alpha$ can be either the scaling or the wavelet function
\begin{equation}
    \scalewave^{\alpha_p,n}_{j_p,l_p} \mydef 
    \left\{
	\begin{array}{lll}
	    \scaling^n_{j_p,l_p}	&\mbox{ if }\alpha_p = 0\\
	    \wavelet^n_{j_p,l_p}	&\mbox{ if }\alpha_p = 1
	\end{array}
    \right.
\end{equation}
The wavelet functions for the $d$-dimensional space can thus be expressed as
\begin{equation}
    \waveletnd^{\alpha,n}_{\bs{j}, \bs{l}}(\bs{x}) =
    \prod_{p=1}^d\scalewave^{\alpha_p,n}_{j_p,l_p}(x_p)
\end{equation}
Where the total $\alpha$ index on $\waveletnd$ separates the $2^d$ different
possibilities of combining scaling/wavelet functions with the same index
combination $\bs{j} = (j_0,j_1,\dots,j_k)$. $\alpha$ is given by the 
binary expansion ($\alpha_d\cdots\alpha_1\alpha_0$) and thus runs from $0$ 
to $2^d-1$. By closer inspection we see that $\alpha=0$ recovers the pure 
scaling function
\begin{equation}
    \waveletnd^{0,n}_{\bs{j},\bs{l}}(\bs{x}) \equiv
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
and we will keep the notation $\scalingnd^n_{\bs{j},\bs{l}}$ for the
scaling function, and exclude the $\alpha=0$ term in the wavelet notation
when treating multi-dimensional functions.

We can immediately see that the dimensionality of the wavelet space is higher
than the scaling space on the same scale $n$, specifically $2^d-1$ times
higher. This must be the case in order to conserve the 
dimensionality through the equation
\begin{equation}
    \scalingspace{n+1,d} = \scalingspace{n,d} \oplus \waveletspace{n,d}
\end{equation}
since $dim(\scalingspace{n+1,d}) = 2^d dim(\scalingspace{n,d})$.

As for the mono-dimensional case we can define filter matrices that transform
the scaling functions at scale $n+1$, 
$\lbrace\scalingnd^{n+1}_{\bs{j},\bs{l}}\rbrace$, 
into scaling and wavelet functions at scale $n$, 
$\lbrace\waveletnd^{\alpha,n}_{\bs{j},\bs{l}}\rbrace_{\alpha=0}^{2^d-1}$. 
Details of this construction can be found in the supporting information of
Frediani \etal\cite{Frediani:2013p1143}, where the corresponding matrices are 
shown to be tensor products of the mono-dimensional matrices. This means that 
the multi-dimensional wavelet transform can be done by consecutive application 
of $d$ mono-dimensional filters. Much more is said on multi-dimensional MRAs
and wavelet transforms in Tymczak \etal\cite{Tymczak:2002}.

\section{Function representation}
In this section we will describe how to represent functions in the multiwavelet
basis, as well as how to perform simple arithmetic operations.

\subsection{Function projection}
We introduce the projection operator \scalingproj{n} onto the basis 
$\lbrace\scaling^n_{j,l}\rbrace$ that span the scaling space \scalingspace{n}
\begin{equation}
    \label{eq:scaling_exp}
    f(x) \approx \scalingproj{n}f(x) \mydef \scalingrep{f}{n}(x) =
	\sum_{l=0}^{2^n-1}\sum_{j=0}^k\scalingcoef^{n,f}_{j,l}\scaling^n_{j,l}(x)
\end{equation}
where the expansion coefficients $\scalingcoef^{n,f}_{j,l}$, the so-called 
\emph{scaling} coefficients, are obtained by the projection integral
\begin{equation}
    \label{eq:scaling_coef}
    \scalingcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\scaling^n_{j,l}(x)\ud x
\end{equation}
The accuracy of this approximation is determined by the scale $n$ at which the
projection is performed: the higher the scale, the better the approximation.

\subsection{Multiresolution functions}
We can also introduce the projection operator \waveletproj{n} that projects
onto the wavelet basis $\lbrace\wavelet^n_{j,l}\rbrace$ of the space 
\waveletspace{n}
\begin{equation}
    \label{eq:wavelet_exp}
    \waveletproj{n}f(x) \mydef \waveletrep{f}{n}(x) =
    \sum_{l=0}^{2^n-1}\sum_{j=0}^k\waveletcoef^{n,f}_{j,l}\wavelet^n_{j,l}(x)
\end{equation}
where the \emph{wavelet} coefficients are given as
\begin{equation}
    \label{eq:wavelet_coef}
    \waveletcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\wavelet^n_{j,l}(x)\ud x
\end{equation}
According to Eq.~(\ref{eq:MRAcomplement}) we have the following relationship 
between the projection operators
\begin{equation}
    \label{eq:proj_rel}
    \scalingproj{n+1} = \scalingproj{n} + \waveletproj{n}
\end{equation}
which means that the wavelet projection should not be regarded as an approximation 
of the function $f$, but rather the difference between two approximations
\begin{equation}
    \label{eq:rep_rel}
    \waveletrep{f}{n} = \waveletproj{n} f = (\scalingproj{n+1}-\scalingproj{n})f = 
	\scalingrep{f}{n+1} - \scalingrep{f}{n}
\end{equation}
This means that the wavelet projection \waveletrep{f}{n} can be used as a measure 
of the accuracy of the scaling projection \scalingrep{f}{n}, provided that the 
projection sequence is converging, $\lim_{n\rightarrow\infty} \scalingrep{f}{n} = f$, 
which will be the case for square integrable functions\cite{Alpert:1993p5460}.
By recursive application of Eq.~(\ref{eq:rep_rel}) a given approximation $f^N$ can 
be expressed as the much coarser approximation \scalingrep{f}{0} with a number 
of wavelet corrections
\begin{align}
    \label{eq:highres}
    f(x)    &\approx \scalingrep{f}{N}(x)\\
    \label{eq:multires}
	    &= \scalingrep{f}{0}(x) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(x)
\end{align}
These equivalent representations are the high-resolution and multi-resolution 
approximations, respectively, of the function $f$. The forward and backward
wavelet transforms of Eq.~(\ref{eq:twoscalerelations}) allow us to change between 
the representations of Eqs.~(\ref{eq:highres}) and (\ref{eq:multires}).

In principle it is possible to perform wavelet reconstructions \emph{beyond} the
finest scale $N$ in the function representation \scalingrep{f}{N}. In this case
the wavelet contributions $\waveletvec_l^n$ in the inverse of 
Eq.~(\ref{eq:twoscalerelations}) are zero, and no additional information is
given to the scaling representation. However, the size of the scaling basis is 
doubled when the scale is increased by one, and the effect of such a wavelet 
reconstruction is that we get an \emph{oversampled} representation of the 
function. This upsampling, usually denoted by the operator 
$\uparrow(\scalingrep{f}{N})$, is often necessary in practical implementations, 
as it is usually convenient to relate different function representations at a 
\emph{common} scale that might be beyond the finest scale of one of the individual 
representations.

We also have the downsampling operator $\downarrow(\scalingrep{f}{N})$ that reduces
the size of the basis, which means that information is thrown away in the process.
In particular, a downsampling correspond to a projection onto the next coarser
scaling space, and we have $\downarrow(\scalingrep{f}{N}) \equiv \scalingrep{f}{N-1}$.
Note that the upsampling and downsampling operators do not commute, as
\begin{align}
    \downarrow(\uparrow(\scalingrep{f}{N})) &= \scalingrep{f}{N}\\ 
    \uparrow(\downarrow(\scalingrep{f}{N})) &= \uparrow(\scalingrep{f}{N-1}) 
	\neq \scalingrep{f}{N}
\end{align}

\subsection{Multiresolution functions in $d$ dimensions}
The multi-dimensional function representation is obtained similarly to
Eq.~(\ref{eq:scaling_exp}) by projection onto the multi-dimensional basis
Eq.~(\ref{eq:multidimscaling})
\begin{equation}
    \label{eq:scaling_exp_nd}
    f(\bs{x}) \approx \scalingrep{f}{n}(\bs{x}) = \sum_{\bs{l}}
    \sum_{\bs{j}} \scalingcoef^{n,f}_{\bs{j},\bs{l}} 
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the sums are over all possible translation vectors 
$\bs{l} = (l_1,\dots,l_d)$ for $0\leq l_p\leq 2^n-1$, and all possible 
scaling function combinations $\bs{j} = (j_1,\dots,j_d)$ for 
$0\leq j_p\leq k$. The scaling coefficients are obtained by the
multi-dimensional integral
\begin{equation}
    \label{eq:waveletproj_nd}
    \scalingcoef^{n,f}_{\bs{j},\bs{l}} \mydef
    \int_{[0,1]^d}f(\bs{x})\scalingnd^n_{\bs{j},
    \bs{l}}(\bs{x})\ud \bs{x}
\end{equation}
The wavelet components are given as
\begin{equation}
    \waveletrep{f}{n}(\bs{x}) = \sum_{\bs{l}} \sum_{\bs{j}} 
    \sum_{\alpha=1}^{2^d-1}\waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} 
	\waveletnd^{\alpha,n}_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the $\bs{l}$ and $\bs{j}$ summations are the same as in
Eq.~(\ref{eq:scaling_exp_nd}), and the $\alpha$ sum is over all combinations of
scaling/wavelet functions (excluding the pure scaling $\alpha=0$).
The expansion coefficients are obtained by the multi-dimensional projection
\begin{equation}
    \waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} \mydef
	\int_{[0,1]^d}f(\bs{x})\waveletnd^{\alpha,n}_{\bs{j},
	\bs{l}}(\bs{x})d\bs{x}
\end{equation}
We can again approximate the function $f(\bs{x})$ at scale $N$ and 
decompose it into its multiresolution components
\begin{equation}
    f(\bs{x}) \approx \scalingrep{f}{N}(\bs{x}) = 
    \scalingrep{f}{0}(\bs{x}) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(\bs{x})
\end{equation}

\subsection{Addition of functions}\label{sec:addition}
The addition of functions in the multiwavelet basis is quite straightforward, 
as it is represented by the mappings
\begin{align}
    \label{eq:addmap}
    \begin{split}
	\scalingspace{n} + \scalingspace{n} &\rightarrow \scalingspace{n}\\
	\waveletspace{n} + \waveletspace{n} &\rightarrow \waveletspace{n}
    \end{split}
\end{align}
This basically means that the projection of the sum equals the sum of the
projections. In the polynomial basis this is simply the fact that the sum of
two $k$-order polynomials is still a $k$-order polynomial.

\subsection{Multiplication of functions}\label{sec:multiplication}
Multiplication of functions in the multiwavelet basis is somewhat more
involved than addition. The reason for this is that, in contrast to
Eq.~(\ref{eq:addmap}), the product is represented by the mapping
\begin{equation}
    \label{eq:multmap}
    \scalingspace{n} \times \scalingspace{n} \rightarrow V^n_{2k}
\end{equation}
This means that the product of two functions falls outside of the MRA and needs 
to be projected back onto the scaling space sequence. Following 
Beylkin~\cite{Beylkin:1992} we can say that the product of two functions on a given 
scale ''spills over'' into the finer scales
\begin{equation}
    \label{eq:multmapinf}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{\infty} \waveletspace{n'}
\end{equation}
Working with a finite precision it is desirable to make the product as accurate
as each of the multiplicands. This is done by terminating the sum in
Eq.~(\ref{eq:multmapinf}) at some sufficiently large scale $N>n$
\begin{align}
    \label{eq:multmapN}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{N-1} \waveletspace{n'} &= \scalingspace{N}
\end{align}
As the finest scale $N$ required in the product in general will be higher than the
finest scale $n$ in each of the multiplicands, it is convenient to perform the
multiplication on oversampled representations of the multiplicands obtained by
$N-n$ upsamplings.

\section{Operator representation}\label{sec:operator}
In this section we discuss the multiresolution analysis of a general operator $T$
\begin{equation}
    g(x) = [Tf](x)
\end{equation}
and we describe two different multiresolution representation of the operator: the 
so-called standard and non-standard representations. The difference between 
the two is largely a matter of implementation, as they are mathematically equivalent,
but as we will see below, the non-standard form leads to considerably simpler algorithms, 
especially in the multi-dimensional implementation. In the standard representation the
operator couples all length scales in all dimensions, leading to a very complicated
operator structure, while in the non-standard representation the different scales are 
decoupled in the operator application, while the interaction between scales are handled 
by a post-processing step.

An essential feature in the discussion of operators in the multiresolution framework
is the number of vanishing moments of the chosen basis. This property leads to 
effectively sparse representations of certain operators (in the sense that sparse
representations can be obtained to a given accuracy by a priori thresholding of small 
coefficients), and fast (linear-scaling) algorithms can be obtained for the operator 
application.

A necessary assumption for an efficient implementation of a multi-dimensional operator
is that it is separable in the Cartesian coordinates. This, combined with the tensor
structure of the multiwavelet basis, ensures that the multi-dimensional operator 
application can be performed using mono-dimensional algorithms, and that the exponential
scaling in the dimension is significantly reducecd. This assumption does not
limit the applicability of the method on real-world problems, as many important 
non-separable operators in physics can be made separable to a finite, but arbitrary
precision.

\subsection{Operator projection}
Working in the multiresolution analysis, the operator is applied to the projection 
of $f$ at a given scaling space \scalingspace{n}
\begin{equation}
    \hat{g}(x) = [T\scalingproj{n}f](x) 
\end{equation}
and we are looking for the projected solution
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}f](x) 
\end{equation}
Using the fundamental property of projection operators 
$\scalingproj{n}\scalingproj{n} = \scalingproj{n}$ we get
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}\scalingproj{n}f](x) 
\end{equation}
and we can represent the full operator application on scale $n$
\begin{equation}
    \scalingrep{\hat{g}}{n} (x) =\ \T{n}{n}\scalingrep{f}{n} (x)
\end{equation}
where the projection of the operator $T$ at the scaling space \scalingspace{n}
is defined as
\begin{equation}
    \label{eq:defTn}
    \T{n}{n} \mydef \scalingproj{n} T \scalingproj{n}
\end{equation}
This operation should be performed at a scale $N$ where the overall accuracy of
the representations are satisfactory, and we can assume that
\begin{equation}
    \scalingrep{\hat{g}}{N} \approx \scalingrep{g}{N} \mydef (Tf)^N \approx g
\end{equation}
Algorithms for how to achieve this accuracy is presented in chapter 
\ref{chap:implementation}.

\subsection{Multiresolution operators}
Making use of Eqs.~(\ref{eq:defTn}) and (\ref{eq:proj_rel}) we can decompose the 
scaling representation of the operator at scale $n+1$ into scaling and wavelet contributions
at the next coarser scale
\begin{align}
    T	&\approx    \PTP{n+1}{n+1}\\
	&=	    \big(\scalingproj{n} + \waveletproj{n}\big) T 
		    \big(\scalingproj{n} + \waveletproj{n}\big)\\
	&=	    \PTP{n}{n} +\ \PTQ{n}{n} +\ \QTP{n}{n} +\ \QTQ{n}{n}
\end{align}
and we simplify the notation with the following definitions, including a 
generalization of the definition in Eq.~(\ref{eq:defTn})
\begin{eqnarray}
    \label{eq:defABCT}
    \begin{split}
    	\A{n}{n'} &\mydef& \QTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \waveletspace{n}\\
	\B{n}{n'} &\mydef& \QTP{n}{n'}&:& \scalingspace{n'}\rightarrow \waveletspace{n}\\
	\C{n}{n'} &\mydef& \PTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \scalingspace{n}\\
	\T{n}{n'} &\mydef& \PTP{n}{n'}&:& \scalingspace{n'}\rightarrow \scalingspace{n}
    \end{split}
\end{eqnarray}
leading to the relation
\begin{equation}
    \label{eq:ABCT}
    \T{n+1}{n+1} =\ \T{n}{n}\ +\ \C{n}{n}\ +\ \B{n}{n}\ +\ \A{n}{n}\ 
\end{equation} 
The motivation for such a decomposition of the operator lies in the vanishing moments
of the basis. The $A$, $B$ and $C$ parts of the operator involves projections into 
the wavelet basis, which has the property of vanishing moments, and we will see later
that this leads to sparse representations of certain operators.

The decomposition in Eq.~(\ref{eq:ABCT}) can be continued recursively, and by this 
introduce more sparsity into the operator, and there are two ways to proceed in order 
to achieve this. In the following both the standard and the non-standard form of the 
multiresolution operator will be presented.

\subsection{Standard representation}
The standard representation is the straightforward matrix realization of the operator 
in the multiresolution basis. In order to obtain this representation we start with the 
matrix representation in the scaling basis at scale $N$
\begin{equation}
\begin{split} 
    \label{eq:Tmatrix}
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$
	\ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \T{N}{N}
	\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $}$
	\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right) 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ f^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    = 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ g^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
This matrix can be decomposed into four submatrices according to Eq.~(\ref{eq:ABCT})
while the functions are decomposed into scaling and wavelet contributions at scale $N-1$
\begin{align}
    \scalingrep{f}{N} &= \scalingrep{f}{N-1} + \waveletrep{f}{N-1}\\
    \scalingrep{g}{N} &= \scalingrep{g}{N-1} + \waveletrep{g}{N-1}
\end{align}
According to Eq.~(\ref{eq:defABCT}) \T{n}{n} and \C{n}{n} produce the scaling part of $g$, 
acting on the scaling and wavelet parts of $f$, respectively. Similarly, \A{n}{n} and 
\B{n}{n} produce the wavelet part of $g$, by acting on the wavelet and scaling parts of $f$, 
respectively. The matrix equation Eq.~(\ref{eq:Tmatrix}) can thus be decomposed as
\begin{equation}
\begin{split}
    \label{eq:ABCTmatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \T{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \C{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \B{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \A{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split} 
\end{equation}
where the size of the total matrix is unchanged. We can now do the same
decomposition of \T{N-1}{N-1} into submatrices at scale $N-2$. The
function components \scalingrep{f}{N-1} and \scalingrep{g}{N-1} need to be 
decomposed as well, so to keep everything consistent, the \B{N-1}{N-1} and 
\C{N-1}{N-1} parts of the operator will have to be transformed accoringly. 
To proceed from here we need the following relations
\begin{align}
    \nonumber
    \B{n}{n}	&= \QTP{n}{n}\\
    \nonumber
		&= \waveletproj{n}T(\scalingproj{n-1}+\waveletproj{n-1})\\
    \nonumber
		&= \QTP{n}{n-1} + \QTQ{n}{n-1}\\
    \label{eq:Bdecomp}
		&=\ \B{n}{n-1} +\ \A{n}{n-1}
\end{align}
and similarly for the $C$ block
\begin{equation}
    \label{eq:Cdecomp}
    \C{n}{n} =\ \C{n-1}{n} +\ \A{n-1}{n}
\end{equation}
which is the change in the operator that is taking place when we decompose 
\scalingrep{f}{n} into $\scalingrep{f}{n-1} + \waveletrep{f}{n-1}$ and \scalingrep{g}{n} 
into $\scalingrep{g}{n-1} + \waveletrep{g}{n-1}$. The matrix equation now turns into
\begin{equation}
\begin{split} 
\label{eq:Smatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{c|c}
	    $\multirow{3}{*}{\T{N-2}{N-2}}$&
	    $\multirow{3}{*}{\C{N-2}{N-2}}$\\ &\\ &\\
	    \hline
	    $\multirow{3}{*}{\B{N-2}{N-2}}$&
	    $\multirow{3}{*}{\A{N-2}{N-2}}$\\ &\\ &\\
	\end{array} &
	\begin{array}{c}
	    $\multirow{3}{*}{$\ \ \ \ \C{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	    \hline
	    $\multirow{3}{*}{$\ \ \ \ \A{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{c|c}
	    $\multirow{6}{*}{\B{N-1}{N-2}}$&
	    $\multirow{6}{*}{\A{N-1}{N-2}}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \A{N-1}{N-1}\ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{3}{*}{\scalingrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
    	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
We can continue this transformation recursively until we reach the coarsest scale. 
Symbolically, we can do the decomposition of Eq.~(\ref{eq:ABCT}) by recursive 
application of itself as well as Eqs.~(\ref{eq:Bdecomp}) and (\ref{eq:Cdecomp}), 
where we gradually introduce more $A$-character into the operator
\begin{eqnarray}
    \nonumber
    \T{N}{N}	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\ \C{n}{n} + 
		    \sum_{n=0}^{N-1}\ \B{n}{n} + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \nonumber
	    	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\Big(\ \C{0}{n} + 
		    \sum_{n'<n}\ \A{n'}{n}\Big) +\\
    \nonumber
		&&  \sum_{n=0}^{N-1}\Big(\ \B{n}{0} + 
		    \sum_{n'>n}\ \A{n'}{n}\Big) + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \label{eq:MRoperS}
		&=\ ^0T^0 + 
		&   \sum_{n=0}^{N-1}\ \Big(\ \C{0}{n} +\  \B{n}{0} +
		    \sum_{n'=0}^{N-1}\ \A{n}{n'} \Big)
\end{eqnarray}
This multiresolution matrix representation of the operator is called the standard 
representation.

\subsection{Non-Standard representation}
While the standard form of the operator given in Eq.~(\ref{eq:MRoperS}) does
lead to sparse representations, it gives rise to rather complicated algorithms,
especially in several dimensions, as it couples all scales in the problem. 
Beylkin~\etal \cite{Beylkin:1991} introduced a different approach, which they called
the non-standard representation, where the scales are explicitly separated, by
organizing the operator as a collection of triples
\begin{equation}
    \T{N}{N} =\ \T{0}{0}\ +\ \sum_{n=0}^{N-1} \big(\ \A{n}{n}\ +\ \B{n}{n}\ +\ \C{n}{n}\ \big) 
\end{equation}
where each triple $(\ \A{n}{n},\ \B{n}{n},\ \C{n}{n})$ corresponds to the interaction
at a particular scale $n$. The interaction \emph{between} different length scales are
not explicitly treated in this representation, and needs to be accounted for in
a post-prosessing step. In order to achieve this separation of scales some redundancy
is necessary in the function representations for $f$ and $g$, as we need to keep the 
scaling projections at \emph{all} scales. The operator matrix that is applied to the 
function will in this case be
\begin{align}
	\small
	\label{eq:NSmatrix}
	\begin{split}
	\left(
	\begin{array}{c|c}
		\begin{array}{c|c}
			$\multirow{4}{*}{\T{N-1}{N-1}}$&
			$\multirow{4}{*}{\C{N-1}{N-1}}$
			\\ &\\ &\\ &\\ \hline
			$\multirow{4}{*}{\B{N-1}{N-1}}$&
			$\multirow{4}{*}{\A{N-1}{N-1}}$
			\\ &\\ &\\ &\\
		\end{array}
		&
		\begin{array}{cc}
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
		\end{array}\\\hline
		\begin{array}{c}
			\begin{array}{cc}
				&\\&
			\end{array}\\
			\begin{array}{cc}
				&\\&
			\end{array}\\
		\end{array}
		&
		\begin{array}{c|c}
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \C{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\\hline
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \B{N-1}{N-1}\ \ \ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \A{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\
		\end{array}
	\end{array}
	\right)	\left(
	\begin{array}{c}
		$\multirow{4}{*}{\scalingrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{4}{*}{\waveletrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\\hline
		$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\end{array}
	\right)	
	%&= \left(
	%\begin{array}{c}
	%	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\\hline
	%	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	%\end{array}
	%\right)
	\end{split}
\end{align}
and although the total matrix has grown in size, this representation leads to 
straightforward adaptive algorithms, as the operator can be applied one scale
at the time, starting from the coarsest (usually $n=0$). As pointed out above, 
this does not directly account for the interaction between scales, but this can
be included by a series of wavelet transforms on parts of the result. This is 
described fully in the implementation part in chapter \ref{chap:implementation}. 
The post-processing wavelet transforms require $O(N)$ operations, and provided
sparse $A$, $B$ and $C$ parts of the operator, the complete non-standard 
application scales as $O(N)$, in contrast to the standard form, where 
scale-to-scale interactions are treated explicitly, which has a formal 
$O(N log N)$ scaling \cite{Beylkin:1991}.

\subsection{Integral operator}
Multiwavelets were originally introduced for their effectively sparse 
representation of certain integral operators, in particular operators with 
non-oscillatory kernels that are analytic except along a finite set of 
curves~\cite{Alpert:1993p5460}. To be more specific, we consider one-dimensional 
operators on the form
\begin{equation}
    \label{eq:intop1d}
    [Tf](x) = \int K(x,y) f(y) \ud y
\end{equation}
The sparsity of the operator representation follows under certain conditions 
on the integral kernel $K$, which is discussed below. We start, however, by 
expanding the kernel in the multiwavelet basis
\begin{equation}
    \label{eq:kernelexp}
    K^n(x,y) = \sum_{l,m} \sum_{i,j}
	\big[\tau^n_{lm}\big]_{ij} 
	\scaling_{i,l}^n(x)
	\scaling_{j,m}^n(y)
\end{equation}
where the expansion coefficients are given by the integrals
\begin{equation}
    \label{eq:taudef}
    \big[\tau^n_{lm}\big]_{ij} = \int\int
	K(x,y)\scaling^n_{i,l}(x)\scaling^n_{j,m}(y)\ud x \ud y
\end{equation}
Inserting Eq.~(\ref{eq:kernelexp}) into Eq.~(\ref{eq:intop1d}) yields
\begin{align}
    \T{n}{n}\scalingrep{f}{n}(x) &= \int\left(\sum_{l,m} \sum_{i,j}
	\big[\tau^n_{lm}\big]_{ij}
	\scaling^n_{i,l}(x)
	\scaling^n_{j,m}(y)
	\right)f(y)\ud y\\
	&= \sum_{l,m} \sum_{i,j}
	\big[\tau^n_{lm}\big]_{ij}
	\scaling^n_{i,l}(x)
	\int f(y)\scaling^n_{j,m}(y)\ud y
\end{align}
where the last integral is recognized as the vector of scaling coefficients of $f$ 
from Eq.~(\ref{eq:scaling_coef})
\begin{equation}
    \label{eq:Tact}
    \T{n}{n}\scalingrep{f}{n}(x) = \sum_{l,m} \sum_{i,j} 
	\big[\tau^n_{lm}\big]_{ij}
	\scaling^n_{i,l}(x)
	\scalingcoef^{n,f}_{j,m}
\end{equation}
We can now identify $\taumat^n_{lm}$ as the matrix elements of \T{n}{n} 
and Eq.~(\ref{eq:Tact}) is Eq.~(\ref{eq:Tmatrix}) written explicitly. 
Similarly, we define \alphamat, \betamat and \gammamat as the matrix 
elements of $A$, $B$ and $C$, respectively
\begin{align}
    \label{eq:tcbadef}
    \big[\alpha^n_{lm}\big]_{ij} &= \int\int
	K(x,y)\wavelet^n_{i,l}(x)\wavelet^n_{j,m}(y)\ud x \ud y\\
    \big[\beta^n_{lm}\big]_{ij} &= \int\int
	K(x,y)\wavelet^n_{i,l}(x)\scaling^n_{j,m}(y)\ud x \ud y\\
    \big[\gamma^n_{lm}\big]_{ij} &= \int\int
	K(x,y)\scaling^n_{i,l}(x)\wavelet^n_{j,m}(y)\ud x \ud y
\end{align}
which act on the function representations of $f$ in the following way
\begin{align}
    \label{eq:Aact}
    \A{n}{n}\waveletrep{f}{n}(x) &= \sum_{l,m} \sum_{i,j}
	\big[\alpha^n_{lm}\big]_{ij}
        \wavelet^n_{i,l}(x) 
	\waveletcoef^{n,f}_{j,m}\\
    \label{eq:Bact}
    \B{n}{n}\scalingrep{f}{n}(x) &= \sum_{l,m} \sum_{i,j}
	\big[\beta^n_{lm}\big]_{ij}
        \wavelet^n_{i,l}(x) 
	\scalingcoef^{n,f}_{j,m}\\
    \label{eq:Cact}
    \C{n}{n}\waveletrep{f}{n}(x) &= \sum_{l,m} \sum_{i,j}
	\big[\gamma^n_{lm}\big]_{ij}
        \scaling^n_{i,l}(x) 
	\waveletcoef^{n,f}_{j,m}
\end{align}
As was mentioned above, the motivation for decomposing the operator into $A$, $B$
and $C$ terms is that these matrices will be sparse for certain operators. Suppose 
that the integral kernel in Eq.~(\ref{eq:intop1d}) satisfy the estimates
\begin{align}
    |K(x,y)| &\leq \frac{1}{|x-y|}\\
    |\partial_x^M K(x,y)|+ |\partial_y^M K(x,y)| &\leq \frac{C_M}{|x-y|^{M+1}}
\end{align}
for some $M\geq1$. Such operators are called Calderon-Zygmund operators, and
include both the Poisson and bound-state Helmholtz operators which is discussed 
in detail below. Beylkin \etal~\cite{Beylkin:1991} 
shows that in a basis with $M$ vanishing moments, the wavelet 
components \alphamat, \betamat\ and \gammamat\ will be bounded as
\begin{equation}
    \label{eq:operbandwidth}
    \|\alphamat_{lm}\|_2 + \|\betamat_{lm}\|_2 + \|\gammamat_{lm}\|_2 
	\leq \frac{C_M}{1+|l-m|^{M+1}}
\end{equation}
where the expression has been adapted to a multiwavelet setting using the matrix
2-norm. This means that within a given accuracy, all contributions beyond a certain 
spatial separation $|l-m|$ can be set to zero, leading to operators that are 
banded along the diagonal.

\subsection{Derivative operator}\label{sec:diff_oper}
Alpert \etal~\cite{Alpert:2002p149} described how to construct derivative operators in
the multiwavelet basis. Since the basis is discontinuous, there does not exist a
unique representation of the derivative operator. This non-uniqueness
appears as two adjustable parameters that handles boundary conditions at the
discontinuous merging point between basis functions. The representation can be
viewed as the straightforward differentiation of the basis functions at the 
\emph{interior} of each interval, combined with a finite difference representation
\emph{across} intervals.

The matrix representation of the operator $T=d/dx$ is formally given as
\begin{align}
    \big[\tau_{lm}^{n,n}\big]_{ij} &= \int_{2^{-n}l}^{2^{-n}(l+1)} 
        \scaling_{i,l}^n(x) T \scaling_{j,m}^n(x) \ud x\\
	&= 2^n \int_0^1 \scaling_i(x) T \scaling_j(x-(l-m)) \ud x
\end{align}
however, for derivative operators, this integral is not absolutely convergent. 
Because of the disjoint support of the basis functions, it is immidiately clear that
there will be no interaction beyond the neighboring interval, and $\taumat_{lm} = 0$ for
$|l-m| > 1$. The case $|l-m| = 1$ needs to be treated with care, since there are
boundary effects to consider even if the basis functions are non-overlapping. This
becomes apparent if we look at the scaling coefficients of the derivative 
\scalingrep{f'}{n} of a function \scalingrep{f}{n} represented in the scaling basis 
at scale $n$
\begin{equation}
    \scalingcoef_{i,l}^{n,f'} = \int_{2^{-n}l}^{2^{-n}(l+1)} 
	\scaling_{i,l}^n(x) \frac{\ud}{\ud x} \scalingrep{f}{n}(x) \ud x 
\end{equation}
Integration by parts now introduces a boundary term
\begin{align}
    \scalingcoef_{i,l}^{n,f'} 
	&= \scaling_{i,l}^n(x)\scalingrep{f}{n}(x)\Big|_{2^{-n}l}^{2^{-n}(l+1)}
	    - \int_{2^{-n}l}^{2^{-n}(l+1)} 
	    \scalingrep{f}{n}(x)
	    \frac{\ud}{\ud x} \scaling_{i,l}^n(x) \ud x\\
    \label{eq:diff_boundary}
	&= 2^{n/2} \Big[\scalingrep{f}{n}(2^{-n}(l+1))\scaling_i(1) - 
	    \scalingrep{f}{n}(2^{-n}l)\scaling_i(0)\Big] - 2^n\sum_{j=0}^{k} 
	    K_{ij}\scalingcoef_{j,l}^{n,f}
\end{align}
where the matrix $K$ is defined
\begin{equation}
    K_{ij} = \int_0^1 \scaling_j(x) \frac{\ud}{\ud x} \scaling_i(x) \ud x
\end{equation}
We see in Eq.~(\ref{eq:diff_boundary}) that the function representation 
\scalingrep{f}{n} needs to be evaluated precisely at the discontinuities of the
basis where the function value is not well defined. This problem is circumvented
by interpolating between the function values obtained at both sides of the boundary
\begin{equation}
    \scalingrep{f}{n} = a \scalingrep{f}{n}_{-} + b \scalingrep{f}{n}_{+}
\end{equation}
where $a$ and $b$ are adjustable parameters. In the Haar basis (piecewise constants)
this reduces to a finite difference definition of the derivative, with the choice 
$a=b=1/2$ corresponding to central difference, and $a=1,b=0$ and $a=0,b=1$ 
corresponding to forward and backward differences, respectively. With the choice
$a=b=0$ no boundary effects are treated, and the derivative is obtained by a 
straightforward piecewise derivative of the polynomial basis. 

%This inevitably leads
%to a reduction of the approximation order as the basis is effectively reduced from
%order $k$ to order $k-1$. The way to maintain the high approximation order is then
%to use the boundary information ($a=b=1/2$) whenever the approximation order of
%the finite differences ($O((2^{-n})^2)$) is at least as high as the approximation 
%of the basis ($O(2^{-n(k-1)})$). This is important to consider in adaptive function 
%representations, where parts of the function can be represented at relatively coarse 
%scales, where the inclusion of the boundary term can severly derate the approximation 
%order of the differentiated function.

\subsection{Multiresolution operators in $d$ dimensions}
We assume that we have a separable representation of a $d$-dimensional
operator $\mathcal{T}$ such that
\begin{equation}
    \mathcal{T} = \bigotimes_{p=1}^d T_p
\end{equation}
where $T_p$ correspond to a one-dimensional operator as described above.
As for the one-dimensional case we have the equation
\begin{equation}
    \scalingrep{g}{n+1} = \bigotimes^d\ \T{n+1}{n+1} \scalingrep{f}{n+1}
\end{equation}
which we can decompose to
\begin{equation}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\bigotimes^d\ \Big(\ \A{n}{n} +\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) 
	\big(\scalingrep{f}{n} + \waveletrep{f}{n}\big)
\end{equation}
and we can simplify the notation in the following way
\begin{align}
    \begin{split}
        \A{n}{n} &= O^{11,n} \qquad \qquad
	\B{n}{n} = O^{10,n} \\
	\C{n}{n} &= O^{01,n} \qquad \qquad
	\T{n}{n} = O^{00,n}
    \end{split}
\end{align}
and the tensor product of the operator can be written
\begin{equation}
    \bigotimes^d\ \Big(\ \A{n}{n}+\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) =
	\sum_{\alpha=0}^{2^d-1} \sum_{\beta=0}^{2^d-1} O^{\alpha,\beta,n}
\end{equation}
where we define
\begin{equation}
    O^{\alpha\beta,n} \mydef \bigotimes_{p}^d\ O^{\alpha_p\beta_p,n}
\end{equation}
with $0 \leq \alpha < 2^d$ and $0 \leq \beta < 2^d$ and $\alpha_p$ and $\beta_p$
are defined by the binary expansion of $\alpha$ and $\beta$ in $d$ dimensions.
We can now obtain a completely equivalent structure as for the mono-dimensional
case
\begin{equation}
    \label{eq:multidimoper}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\Big(\mathcal{A}^n + \mathcal{B}^n + \mathcal{C}^n + \mathcal{T}^n\Big)
	\big(\scalingrep{f}{n} + \waveletrep{f}{n}\big)
\end{equation} 
with the following definitions
\begin{eqnarray}
    \begin{split}
	\mathcal{A}^{n} &\mydef 
        \sum_{\alpha=1}^{2^d-1} \sum_{\beta=1}^{2^d-1} O^{\alpha\beta,n} \qquad
	&\mathcal{B}^{n} &\mydef 
	\sum_{\alpha=1}^{2^d-1} O^{\alpha 0,n} \\
	\mathcal{C}^{n} &\mydef 
	\sum_{\beta=1}^{2^d-1} O^{0 \beta,n}
	&\mathcal{T}^{n} &\mydef O^{00,n}
    \end{split}
\end{eqnarray}
We could now proceed with a further decomposition of the scaling parts of the
operator and functions to the next coarser scale, obtaining the standard
representation of the operator in multiple dimension. It is quite clear that the 
notation (as well as implementation) becomes very complicated in this case, and this
is one of the main motivations for using the non-standard representation of operators,
as the scales are decoupled, and Eq.~(\ref{eq:multidimoper}) applies to each scale
separately.

