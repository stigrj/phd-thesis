\chapter{Mathematical Theory}
Wavelet theory is a rather young field of mathematics, first appearing in the
late 1980s. The initial application was in signal theory \cite{Strang} but in
the early 90s, wavelet-based methods started to appear for the solution of
PDEs and integral equations \cite{Beylkin90}\cite{Alpert93}, and in recent 
years for application in electronic structure calculations
\cite{Harrison}\cite{Niklasson}\cite{Arias}. The presented work is based on the
generalization called multiwavelets, which was introduced by Alpert\cite{Alpert}.

In this chapter a general introduction to multiwavelet theory will be given through 
the concept of multiresolution analysis (MRA)\footnote{Keinert\cite{Keinert} uses 
the term multiresolution \emph{approximation}, but in this work we will use
multiresolution \emph{analysis}, as it is more commonly used in the literature.}.
A detailed description of MRAs can be found in Keinert\cite{Keinert}, from which a 
brief summary of the key issues are given in the following, with the difference that 
we limit our discussion to the unit interval instead of the real line. This work is 
concerned with orthogonal MRA only, and for a description of the general bi-orthogonal 
MRA the reader is referred to Keinerts book. 

\section{Multiresolution Analysis}
A multiresolution analysis of $L^2([0,1])$ is an infinite nested sequence of subspaces
\begin{equation}
    \label{eq:MRA}
    \scalingspace{0}\subset \scalingspace{1}\subset \cdots \subset 
	\scalingspace{n}\subset \cdots \subset L^2([0,1])
\end{equation}
with the following properties
\begin{enumerate}
    \item $\scalingspace{\infty}\ is\ dense\ in\ L^2([0,1])$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(2x) \in \scalingspace{n+1}
    	\ ,\ \forall n \in \mathbb{N}$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(x-2^{-n}l) \in \scalingspace{n}
	\ ,\ \forall n \in \mathbb{N},\ 0 \leq l \leq 2^n - 1.$
    \item There exists a function vector $\scalingvec$ of length $k+1$ in 
	$L^2([0,1])$ so that \[\lbrace \scaling_j(x):\  0\leq j\leq k \rbrace\] 
	forms a basis for \scalingspace{0}. 
\end{enumerate}
This means that if we can construct a basis of \scalingspace{0}, which consists of only
$k+1$ functions, we can construct a basis of \emph{any} space \scalingspace{n}, by
simple compression (by a factor of $2^n$), and translations (to all dyadic
grid points at scale $n$), of the original $k+1$ functions, and by increasing
the scale $n$, we are approaching a complete basis of $L^2([0,1])$. Since $\scalingspace{n}
\subset \scalingspace{n+1}$ the basis functions of \scalingspace{n} can be expanded in the 
basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalescaling}
    \scalingvec^n_l(x) \mydef 2^{n/2}\scalingvec(2^nx-l) = 
	\sum_{l'=0}^{2^n-1} H^{(l')} \scalingvec^{n+1}_{l'}(x)
\end{equation}
where the $H^{(l')}$s are the so-called filter matrices that describes the 
transformation between different spaces \scalingspace{n}. The MRA is called orthogonal if 
\begin{equation}
    \label{eq:orthogonality}
    \langle \scalingvec_l^n(x), \scalingvec_{l'}^n(x) \rangle = \delta_{l,l'}I_{k+1}
\end{equation}
where $I_{k+1}$ is the $(k+1) \times (k+1)$ unit matrix, and $k+1$ is the 
length of the function vector. This orthogonality condition means that the 
functions are orthogonal both within one function vector and through all 
possible translations on one scale, but \emph{not} through the different scales.

Complementary to the nested sequence of subspaces \scalingspace{n}, we can define 
another series of spaces \waveletspace{n} that complements \scalingspace{n} in 
\scalingspace{n+1}
\begin{equation}
    \label{eq:MRAcomplement}
    \scalingspace{n+1} = \scalingspace{n} \oplus \waveletspace{n}
\end{equation}
where there exists another function vector $\waveletvec$ of lenght $k+1$ that, with 
all its translations on scale $n$ form a basis for \waveletspace{n}.
Analogously to Eq.~(\ref{eq:twoscalescaling}) the function vector can be expanded 
in the basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalewavelet}
    \waveletvec^n_l(x) \mydef 2^{n/2}\waveletvec(2^nx-l) = 
	\sum_{l'=0}^{2^n-1} G^{(l')}\scalingvec_{l'}^{n+1}(x)
\end{equation}
with filter matrices $G^{(l')}$. In orthogonal MRA the functions $\waveletvec$ fulfill 
the same othogonality condition as Eq.~(\ref{eq:orthogonality}), and if we combine 
Eq.~(\ref{eq:MRA}) and Eq.~(\ref{eq:MRAcomplement}) we see that they must also be 
orthogonal with respect to different scales
\begin{equation}
    \langle \waveletvec_l^n(x), \waveletvec_{l'}^{n'}(x) \rangle = 
	\delta_{l,l'}\delta_{n,n'}I_{k+1}
\end{equation}
Recursive application of Eq.~(\ref{eq:MRAcomplement}) yields the important relation
\begin{equation}
    \label{eq:MRArecursive}
    \scalingspace{n} = \scalingspace{0} \oplus \waveletspace{0} \oplus \waveletspace{1}
	\oplus \cdots \oplus \waveletspace{n-1}
\end{equation}

\section{Multiwavelets}
There are many ways to choose the basis functions $\scalingvec$ and $\waveletvec$ 
(which define the spanned spaces \scalingspace{n} and \waveletspace{n},
leading to different wavelet families. There is a one-to-one correspondence between 
the basis functions $\scalingvec$ and $\waveletvec$, and the filter matrices $H^{(l)}$ 
and $G^{(l)}$ used in the two-scale relations Eq.~(\ref{eq:twoscalescaling}) and 
Eq.~(\ref{eq:twoscalewavelet}), and most well known wavelet families are defined 
only through their filter coefficients.

In the following we are taking a different approach, which follows the original 
construction of multiwavelets by Alpert\cite{Alpert93}. We define the \emph{scaling 
space} \scalingspace{n} as the space of piecewise polynomials
\begin{equation}
    \begin{array}{rl}
        \displaystyle \scalingspace{n} \mydef & \lbrace f:\ all\ polynomials\ of\ degree\ \leq\ k\\ 
	\displaystyle &\ on\ the\ interval\ (2^{-n}l,2^{-n}(l+1))\\
	\displaystyle &\ for\ 0 \leq l < 2^n,\ f\ vanishes\ elsewhere \rbrace
    \end{array}
\end{equation}
This definition fulfills the conditions for a multiresolution analysis, and if the 
basis is chosen to be orthogonal, the \scalingspace{n} constitutes an \emph{orthogonal} MRA.

\subsection{The scaling basis}
The construction of the scaling functions is quite straightforward; $k+1$ orthogonal 
polynomials are chosen to span the space of polynomials of degree $\leq k$ on the unit 
interval. The total scaling basis for \scalingspace{n} is then obtained by appropriate dilation 
and translation of these functions. One way to construct the basis is to start with the 
standard basis $\lbrace 1,x,x^2,\dots,x^k\rbrace$ and orthonormalize with respect to 
the $L^2$ inner product on the unit interval.

\subsection{The wavelet basis}
The \emph{wavelet space} \waveletspace{n} is defined, according to Eq.~(\ref{eq:MRAcomplement}), 
as the orthogonal complement of \scalingspace{n} in \scalingspace{n+1}. The wavelet basis functions 
of \waveletspace{n} are hence piecewise polynomials of degree $\leq k$ on \emph{each} of the two 
intervals on scale n+1 that overlaps with \emph{one} interval on scale n (but may be 
discontinous in the merging point). In the construction of the wavelet basis these 
piecewise polynomials are made orthogonal to the basis of \scalingspace{n} and to each other, 
and, following Alpert\cite{Alpert93}, a standard Gram-Schmidt orthogonalization is 
employed to construct a basis that meet the necessary orthogonality conditions.

One important property of the wavelet basis is its number of vanishing moments. 
The k-th continuous moment of a function $\wavelet$ is defined as the integral
\begin{equation}
	\mu_k \mydef \int_0^1 x^k\wavelet(x)dx 	
\end{equation}
and the function $\wavelet$ has $M$ vanishing moments if \[\mu_k = 0,\qquad k=0,\dots, M-1 \]
The vanishing moments of the wavelet functions gives information on the
approximation order of the scaling functions. If the wavelet function
$\wavelet$ has $M$ vanishing moments, any polynomial of order $\leq M-1$ can be 
exactly reproduced in the scaling space, and the error in
representing an arbitrary function in the scaling basis is of $M$-th order. 
By construction, $x^i$ is in the space \scalingspace{0} for $0\leq i \leq k$, and since
$\waveletspace{0} \perp \scalingspace{0}$, the first $k+1$ moments of $\wavelet^0_j$ must vanish.

\subsection{Filter relations}
With the multiwavelet basis defined, we can construct the filter
matrices that fulfill the two-scale relations in Eq.(\ref{eq:twoscalescaling}) and
Eq.(\ref{eq:twoscalewavelet}). The details of this construction can be found in
Alpert \etal\cite{Alpert02}, and will not be presented here, but we specifically 
end up with four matrices $H^{(0)}, H^{(1)}, G^{(0)}$ and $G^{(1)}$, whose size and 
content are dependent on the order and type of the chosen scaling functions. 
Eq.~(\ref{eq:twoscalescaling}) and Eq.~(\ref{eq:twoscalewavelet}) thus reduces to
\begin{equation}
    \label{eq:twoscalerelations}
    \begin{pmatrix}
	\wavelet_{l}^{n}\\
	\scaling_{l}^{n}\\
    \end{pmatrix}=
    \begin{pmatrix}
	G^{(1)}&G^{(0)}\\
	H^{(1)}&H^{(0)}\\
    \end{pmatrix}
    \begin{pmatrix}
	\scaling_{2l+1}^{n+1}\\
	\scaling_{2l}^{n+1}\\
    \end{pmatrix}
\end{equation}

\subsection{Multiwavelets in $d$ dimensions}
Multidimensional wavelets are usually constructed by tensor products, where the
scaling space is defined as
\begin{equation}
    \scalingspace{n,d} \mydef \bigotimes^d \scalingspace{n}
\end{equation}
The basis for this $d$-dimensional space is given as tensor products of the
one-dimensional bases.
\begin{equation}
    \label{eq:multidimscaling}
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x}) = 
    \scalingnd^n_{j_1 j_2\dots j_d,l_1 l_2\dots l_d} (x_1,x_2,\dots,x_d) \mydef
    \prod_{i=1}^d \scaling^n_{j_i,l_i}(x_i)
\end{equation}
The number of basis functions on each hypercube $\bs{l}=(l_1,l_2,\dots,l_d)$ 
becomes $(k+1)^d$, while the number of such hypercubes on scale $n$ becomes $2^{dn}$, 
which again means that the total number of basis functions is growing exponentially 
with the number of dimensions.

The wavelet space can be defined using Eq.~(\ref{eq:MRAcomplement})
\begin{equation}
    \label{eq:multidimW}
    \scalingspace{n+1,d} = \bigotimes^d \scalingspace{n+1} = 
	\bigotimes^d (\scalingspace{n} \oplus \waveletspace{n})
\end{equation}
where the pure scaling term obtained when expanding the product on the right
hand side of Eq.~(\ref{eq:multidimW}) is recognized as \scalingspace{n,d}, making the
wavelet space \waveletspace{n,d} consist of all the remaining terms of the product, 
which are terms that contain at least one wavelet space.

To achieve a uniform notation, we can introduce a "generalized" one-dimensional
wavelet function $\lbrace\scalewave_{j,l}^{\alpha,n}\rbrace$ that, depending on 
the index $\alpha$ can be either the scaling or the wavelet function
\begin{equation}
    \scalewave^{\alpha_i,n}_{j_i,l_i} \mydef 
    \left\{
	\begin{array}{lll}
	    \scaling^n_{j_i,l_i}	&\mbox{ if }\alpha_i = 0\\
	    \wavelet^n_{j_i,l_i}	&\mbox{ if }\alpha_i = 1
	\end{array}
    \right.
\end{equation}
The wavelet functions for the $d$-dimensional space can thus be expressed as
\begin{equation}
    \waveletnd^{\alpha,n}_{\bs{j}, \bs{l}}(\bs{x}) =
    \prod_{i=1}^d\scalewave^{\alpha_i,n}_{j_i,l_i}(x_i)
\end{equation}
Where the total $\alpha$ index on $\waveletnd$ separates the $2^d$ different
possibilities of combining scaling/wavelet functions with the same index
combination $\bs{j} = (j_0,j_1,\dots,j_k)$. $\alpha$ is given by the 
binary expansion
\begin{equation}
    \alpha = \sum_{i=1}^d 2^{i-1}\alpha_i
\end{equation}
and thus runs from $0$ to $2^d-1$. By closer inspection we see that $\alpha=0$
recovers the pure scaling function
\begin{equation}
    \waveletnd^{0,n}_{\bs{j},\bs{l}}(\bs{x}) \equiv
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
and we will keep the notation $\scalingnd^n_{\bs{j},\bs{l}}$ for the
scaling function, and exclude the $\alpha=0$ term in the wavelet notation
when treating multidimensional functions.

We can immediately see that the dimensionality of the wavelet space is higher
than the scaling space on the same scale $n$, specifically $2^d-1$ times
higher. This must be the case in order to conserve the 
dimensionality through the equation
\begin{equation}
    \scalingspace{n+1,d} = \scalingspace{n,d} \oplus \waveletspace{n,d}
\end{equation}
since $dim(\scalingspace{n+1,d}) = 2^d dim(\scalingspace{n,d})$.

As for the monodimensional case we can define filter matrices that transform
the scaling functions at scale $n+1$,
$\lbrace\scalingnd^{n+1}_{\bs{j},\bs{l}}\rbrace$, into scaling and 
wavelet functions at scale $n$, $\lbrace\waveletnd^{\alpha,n}_{\bs{j},
\bs{l}}\rbrace_{\alpha=0}^{2^d-1}$. Details of this construction can be
found in \cite{Fossgaard}, where the corresponding matrices are shown to be 
tensor products of the monodimensional matrices. 

\section{Function representation}
In this section we will describe how to represent functions in the multiwavelet
basis, as well as how to perform simple arithmetic operations.

\subsection{Function projection}
We introduce the projection operator \scalingproj{n} onto the basis 
$\lbrace\scaling^n_{j,l}\rbrace$ that span the scaling space \scalingspace{n}
\begin{equation}
    \label{eq:scaling_exp}
    f(x) \approx \scalingproj{n}f(x) \mydef \scalingrep{f}{n}(x) =
	\sum_{l=0}^{2^n-1}\sum_{j=0}^k\scalingcoef^{n,f}_{j,l}\scaling^n_{j,l}(x)
\end{equation}
where the expansion coefficients $\scalingcoef^{n,f}_{j,l}$, the so-called \emph{scaling}
coefficients, are obtained by the projection integral
\begin{equation}
    \label{eq:scaling_coef}
    \scalingcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\scaling^n_{j,l}(x)\ud x
\end{equation}
The accuracy of this approximation is determined by the scale $n$ at which the
projection is performed: the higher the scale, the better the approximation.

\subsection{Multiresolution functions}
We can also introduce the projection operator \waveletproj{n} that projects
onto the wavelet basis $\lbrace\wavelet^n_{j,l}\rbrace$ of the space \waveletspace{n}
\begin{equation}
    \label{eq:wavelet_exp}
    \waveletproj{n}f(x) \mydef \waveletrep{f}{n}(x) =
    \sum_{l=0}^{2^n-1}\sum_{j=0}^k\waveletcoef^{n,f}_{j,l}\wavelet^n_{j,l}(x)
\end{equation}
where the \emph{wavelet} coefficients are given as
\begin{equation}
    \label{eq:wavelet_coef}
    \waveletcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\wavelet^n_{j,l}(x)\ud x
\end{equation}
According to Eq.~(\ref{eq:MRAcomplement}) we have the following relationship 
between the projection operators
\begin{equation}
    \label{eq:proj_rel}
    \scalingproj{n+1} = \scalingproj{n} + \waveletproj{n}
\end{equation}
which means that the wavelet projection should not be regarded as an approximation 
of the function $f$, but rather the difference between two approximations
\begin{equation}
    \waveletrep{f}{n} = \waveletproj{n} f = (\scalingproj{n+1} - \scalingproj{n}) f = 
	\scalingrep{f}{n+1} - \scalingrep{f}{n}
\end{equation}
This means that the wavelet projection \waveletrep{f}{n} can be used as a measure of the accuracy 
of the scaling projection \scalingrep{f}{n}, provided that the projection sequence is converging,
$\lim_{n\rightarrow\infty} \scalingrep{f}{n} = f$, which is the case for square integrable functions.
A given approximation $f^N$ can then be expressed as the much coarser approximation \scalingrep{f}{0}
with a number of wavelet corrections
\begin{align}
    \label{eq:highres}
    f(x)    &\approx \scalingrep{f}{N}(x)\\
    \label{eq:multires}
	    &= \scalingrep{f}{0}(x) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(x)
\end{align}
These equivalent representations are the high-resolution and multi-resolution 
approximations, respectively, of the function $f$. The filter matrices $H^{(0)}, 
H^{(1)}, G^{(0)}$ and $G^{(1)}$ of Eq.~(\ref{eq:twoscalerelations}) allow us to 
change between the representations of Eq.~(\ref{eq:highres}) and 
Eq.~(\ref{eq:multires}), and the disjoint support of the basis functions ensures 
that this is a local transformation.

\subsection{Multiresolution functions in $d$ dimensions}
The multi-dimensional function representation is obtained similarly to
Eq.~(\ref{eq:scaling_exp}) by projection onto the multi-dimensional basis
Eq.~(\ref{eq:multidimscaling})
\begin{equation}
    \label{eq:scaling_exp_nd}
    f(\bs{x}) \approx \scalingrep{f}{n}(\bs{x}) = \sum_{\bs{l}}
    \sum_{\bs{j}} \scalingcoef^{n,f}_{\bs{j},\bs{l}} 
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the sums are over all possible translation vectors 
$\bs{l} = (l_1,\dots,l_d)$ for $0\leq l_i\leq 2^n-1$, and all possible 
scaling function combinations $\bs{j} = (j_1,\dots,j_d)$ for 
$0\leq j_i\leq k$. The scaling coefficients are obtained by the
multi-dimensional integral
\begin{equation}
    \label{eq:waveletproj_nd}
    \scalingcoef^{n,f}_{\bs{j},\bs{l}} \mydef
    \int_{[0,1]^d}f(\bs{x})\scalingnd^n_{\bs{j},
    \bs{l}}(\bs{x})\ud \bs{x}
\end{equation}
The wavelet components are given as
\begin{equation}
    \waveletrep{f}{n}(\bs{x}) = \sum_{\bs{l}} \sum_{\bs{j}} 
    \sum_{\alpha=1}^{2^d-1}\waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} 
	\waveletnd^{\alpha,n}_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the $\bs{l}$ and $\bs{j}$ summations are the same as in
Eq.~(\ref{eq:scaling_exp_nd}), and the $\alpha$ sum is over all combinations of
scaling/wavelet functions (excluding the pure scaling $\alpha=0$).
The expansion coefficients are obtained by the multi-dimensional projection
\begin{equation}
    \waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} \mydef
	\int_{[0,1]^d}f(\bs{x})\waveletnd^{\alpha,n}_{\bs{j},
	\bs{l}}(\bs{x})d\bs{x}
\end{equation}
We can again approximate the function $f(\bs{x})$ at scale $N$ and 
decompose it into its multiresolution components
\begin{equation}
    f(\bs{x}) \approx \scalingrep{f}{N}(\bs{x}) = 
    \scalingrep{f}{0}(\bs{x}) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(\bs{x})
\end{equation}
The $d$-dimensional filter matrices are obtained by tensor products of the
mono-dimensional filters. This means that by the tensor structure of
the multi-dimensional basis, we can perform the wavelet transform one dimension 
at the time, and the work done scales linearly in the dimension. If the full
$d$-dimensional filter matrix had been applied, the work would have scaled
as the power of the dimension, hence limiting the practical use in higher
dimensions. A more rigorous treatment of the multi-dimensional wavelet
transforms can be found in Tymczak\cite{Tymczak}.

\subsection{Addition of functions}
The addition of functions in the multiwavelet basis is quite straightforward, 
since it is represented by the mappings
\begin{align}
    \label{eq:addmap}
    \begin{split}
	\scalingspace{n} + \scalingspace{n} &\rightarrow \scalingspace{n}\\
	\waveletspace{n} + \waveletspace{n} &\rightarrow \waveletspace{n}
    \end{split}
\end{align}
This basically means that the projection of the sum equals the sum of the
projections. In the polynomial basis this is simply the fact that the sum of
two $k$-order polynomials is still a $k$-order polynomial.

\subsection{Multiplication of functions}
Multiplication of functions in the multiwavelet basis is somewhat more
involved than addition. The reason for this is that, in contrast to
Eq.~(\ref{eq:addmap}), the product is represented by the mapping \cite{Beylkin92}
\begin{equation}
    \label{eq:multmap}
    \scalingspace{n} \times \scalingspace{n} \rightarrow V^n_{2k}
\end{equation}
This means that the product of two functions falls outside of the MRA and needs 
to be projected back onto the scaling space sequence. In particular this means that 
the product of two functions on a given scale ''spills over'' into the finer scales
\begin{equation}
    \label{eq:multmapinf}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{\infty} \waveletspace{n'}
\end{equation}
Working with a finite precision it is desirable to make the product as accurate
as each of the multiplicands. This is done by terminating the sum in
Eq.~(\ref{eq:multmapinf}) at some sufficiently large scale $N$
\begin{align}
    \label{eq:multmapN}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{N-1} \waveletspace{n'} &= \scalingspace{N}
\end{align}
Assume now that $n$ is the finest scale present in either of the multiplicands,
and $N>n$ is the finest scale present in the product. An algorithm to determine 
the maximum scale $N$ needed in the result will be presented in section 
\ref{sec:implementation}, and in the following it is simply assumed that $N$ is 
known a priori. We know that
\begin{equation}
    \scalingspace{n} \subset \scalingspace{n+1} \subset \cdots \subset \scalingspace{N}
\end{equation}
which means that the multiplication could just as well have been written 
\begin{equation}
    \scalingspace{N} \times \scalingspace{N} \rightarrow \scalingspace{N}
\end{equation}
where the representations of the multiplicands on scale $N$ is obtained by a
series of backward wavelet transforms. This will result in an \emph{oversampling} 
of the multiplicands which allows us to relate the scaling projections of the product
on scale $N$ to the projections of the multiplicands on the same scale.

Finally, when we have obtained the scaling projection of the product on scale
$N$ we do a wavelet decomposition to obtain wavelet projections on 
the coarser scales. 

\section{Operator representation}
In this section we discuss the multiresolution analysis of a general operator $T$
\begin{equation}
    g(x) = [Tf](x)
\end{equation}
and we describe two different multiresolution representation of the operator: the 
so-called standard and non-standard representations. The difference between 
the two is largely a matter of implementation, as they are mathematecally equivalent,
but as we will see below, the non-standard form leads to considerably simpler algorithms, 
especially in the multi-dimensional implementation. In the standard representation the
operator couples all length scales in all dimensions, leading to a very complicated
operator structure, while in the non-standard representation the different scales are 
decoupled in the operator application, while the interaction between scales are handled 
by a post-processing step.

An essential feature in the discussion of operators in the multiresolution framework
is the number of vanishing moments of the chosen basis. This property leads to 
effectively sparse representations of certain operators (in the sense that sparse
representations can be obtained to a given accuracy by a priori thresholding of small 
coefficients), and fast (linear-scaling) algorithms can be obtained for the operator 
application.

A necessary assumption for an efficient implementation of a multi-dimensional operator
is that it is separable in the Cartesian coordinates. This, combined with the tensor
structure of the multiwavelet basis, ensures that the multi-dimensional operator 
application can be performed using mono-dimensional algorithms, and that the scaling
becomes linear rather than exponential in the dimension. This assumption does not
limit the applicability of the method on real-world problems, as many important 
non-separable operators in physics can be made separable to a finite, but arbitrary
precision. This is discussed further in section \ref{sec:sep_rep}.

\subsection{Operator projection}
Working in the multiresolution analysis, the operator is applied to the projection 
of $f$ at a given scaling space \scalingspace{n}
\begin{equation}
    \hat{g}(x) = [T\scalingproj{n}f](x) 
\end{equation}
and we are looking for the projected solution
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}f](x) 
\end{equation}
Using the fundamental property of projection operators 
$\scalingproj{n}\scalingproj{n} = \scalingproj{n}$ we get
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}\scalingproj{n}f](x) 
\end{equation}
and we can represent the full operator application on scale $n$
\begin{equation}
    \scalingrep{\hat{g}}{n} (x) =\ \T{n}{n}\scalingrep{f}{n} (x)
\end{equation}
where the projection of the operator $T$ at the scaling space \scalingspace{n}
is defined as
\begin{equation}
    \label{eq:defTn}
    \T{n}{n} \mydef \scalingproj{n} T \scalingproj{n}
\end{equation}
This operation should be performed at a scale $N$ where the overall accuracy of
the representations are satisfactory, and we can assume that
\begin{equation}
    \scalingrep{\hat{g}}{N} \approx \scalingrep{g}{N} \mydef (Tf)^N \approx g
\end{equation}
Algorithms for how to achieve this accuracy is presented in chapter 
\ref{chap:implementation}.

\subsection{Multiresolution operators}
Making use of Eqs.~(\ref{eq:defTn}) and (\ref{eq:proj_rel}) we can decompose the 
scaling representation of the operator at scale $n+1$ into scaling and wavelet contributions
at the next coarser scale
\begin{align}
    T	&\approx    \PTP{n+1}{n+1}\\
	&=	    \big(\scalingproj{n} + \waveletproj{n}\big) T 
		    \big(\scalingproj{n} + \waveletproj{n}\big)\\
	&=	    \PTP{n}{n} +\ \PTQ{n}{n} +\ \QTP{n}{n} +\ \QTQ{n}{n}
\end{align}
and we simplify the notation with the following definitions, including a generalization of the
definition in Eq.~(\ref{eq:defTn})
\begin{eqnarray}
    \label{eq:defABCT}
    \begin{split}
    	\A{n}{n'} &\mydef& \QTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \waveletspace{n}\\
	\B{n}{n'} &\mydef& \QTP{n}{n'}&:& \scalingspace{n'}\rightarrow \waveletspace{n}\\
	\C{n}{n'} &\mydef& \PTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \scalingspace{n}\\
	\T{n}{n'} &\mydef& \PTP{n}{n'}&:& \scalingspace{n'}\rightarrow \scalingspace{n}
    \end{split}
\end{eqnarray}
leading to the relation
\begin{equation}
    \label{eq:ABCT}
    \T{n+1}{n+1} =\ \T{n}{n}\ +\ \C{n}{n}\ +\ \B{n}{n}\ +\ \A{n}{n}\ 
\end{equation} 
The motivation for such a decomposition of the operator lies in the vanishing moments
property of the basis. The $A$, $B$ and $C$ parts of the operator involves projections
onto the wavelet basis, which has the property of vanishing moments, and it has been 
shown \cite{Beylkin} that this leads to sparse representations of certain operators.
This will be discussed in more detail in section \ref{sec:int_oper} when the specific 
operators used in this work are treated.

The decomposition in Eq.~(\ref{eq:ABCT}) can be continued recursively, and by this introduce
more sparsity into the operator, and there are two ways to proceed in order to achieve
this. In the following both the standard and the non-standard form of the multiresolution
operator will be presented.

\subsection{Standard representation}
The standard representation is the straightforward matrix realization of the operator in the 
multiresolution basis. In order to obtain this representation we start with the matrix 
representation in the scaling basis at scale $N$
\begin{equation}
\begin{split} 
    \label{eq:Tmatrix}
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$
	\ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \T{N}{N}
	\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $}$
	\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right) 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ f^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    = 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ g^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
This matrix can be decomposed into four submatrices according to Eq.~(\ref{eq:ABCT})
while the functions are decomposed into scaling and wavelet contributions at scale $N-1$
\begin{align}
    \scalingrep{f}{N} &= \scalingrep{f}{N-1} + \waveletrep{f}{N-1}\\
    \scalingrep{g}{N} &= \scalingrep{g}{N-1} + \waveletrep{g}{N-1}
\end{align}
According to Eq.~(\ref{eq:defABCT}) \T{n}{n} and \C{n}{n} produce the scaling part of $g$, 
acting on the scaling and wavelet parts of $f$, respectively. Similarly, \A{n}{n} and 
\B{n}{n} produce the wavelet part of $g$, by acting on the wavelet and scaling parts of $f$, 
respectively. The matrix equation Eq.~(\ref{eq:Tmatrix}) can thus be decomposed as
\begin{equation}
\begin{split}
    \label{eq:ABCTmatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \T{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \C{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \B{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \A{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split} 
\end{equation}
where the size of the total matrix is unchanged. We can now do the same
decomposition of \T{N-1}{N-1} into submatrices at scale $N-2$. The
function components \scalingrep{f}{N-1} and \scalingrep{g}{N-1} need to be 
decomposed as well, so to keep everything consistent, the \B{N-1}{N-1} and 
\C{N-1}{N-1} parts of the operator will have to be transformed accoringly. 
To proceed from here we need the following relations
\begin{align}
    \nonumber
    \B{n}{n}	&= \QTP{n}{n}\\
    \nonumber
		&= \waveletproj{n}T(\scalingproj{n-1}+\waveletproj{n-1})\\
    \nonumber
		&= \QTP{n}{n-1} + \QTQ{n}{n-1}\\
    \label{eq:Bdecomp}
		&=\ \B{n}{n-1} +\ \A{n}{n-1}
\end{align}
and similarly for the $C$ block
\begin{equation}
    \label{eq:Cdecomp}
    \C{n}{n} =\ \C{n-1}{n} +\ \A{n-1}{n}
\end{equation}
which is the change in the operator that is taking place when we decompose 
\scalingrep{f}{n} into $\scalingrep{f}{n-1} + \waveletrep{f}{n-1}$ and \scalingrep{g}{n} 
into $\scalingrep{g}{n-1} + \waveletrep{g}{n-1}$. The matrix equation now turns into
\begin{equation}
\begin{split} 
\label{eq:Smatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{c|c}
	    $\multirow{3}{*}{\T{N-2}{N-2}}$&
	    $\multirow{3}{*}{\C{N-2}{N-2}}$\\ &\\ &\\
	    \hline
	    $\multirow{3}{*}{\B{N-2}{N-2}}$&
	    $\multirow{3}{*}{\A{N-2}{N-2}}$\\ &\\ &\\
	\end{array} &
	\begin{array}{c}
	    $\multirow{3}{*}{$\ \ \ \ \C{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	    \hline
	    $\multirow{3}{*}{$\ \ \ \ \A{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{c|c}
	    $\multirow{6}{*}{\B{N-1}{N-2}}$&
	    $\multirow{6}{*}{\A{N-1}{N-2}}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \A{N-1}{N-1}\ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{3}{*}{\scalingrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
    	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
We can continue this transformation recursively until we reach the coarsest scale. 
Symbolically, we can do the decomposition of Eq.~(\ref{eq:ABCT}) by recursive 
application of itself as well as Eqs.~(\ref{eq:Bdecomp}) and (\ref{eq:Cdecomp}), 
where we gradually introduce more $A$-character into the operator
\begin{eqnarray}
    \nonumber
    \T{N}{N}	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\ \C{n}{n} + 
		    \sum_{n=0}^{N-1}\ \B{n}{n} + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \nonumber
	    	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\Big(\ \C{0}{n} + 
		    \sum_{n'<n}\ \A{n'}{n}\Big) +\\
    \nonumber
		&&  \sum_{n=0}^{N-1}\Big(\ \B{n}{0} + 
		    \sum_{n'>n}\ \A{n'}{n}\Big) + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \label{eq:MRoperS}
		&=\ ^0T^0 + 
		&   \sum_{n=0}^{N-1}\ \Big(\ \C{0}{n} +\  \B{n}{0} +
		    \sum_{n'=0}^{N-1}\ \A{n}{n'} \Big)
\end{eqnarray}
This multiresolution matrix representation of the operator is called the Standard 
representation\cite{somebeylkin}.

\subsection{Non-Standard representation}
While the standard form of the operator given in Eq.~(\ref{eq:MRoperS}) does
lead to sparse representations, it gives rise to rather complicated algorithms,
especially in several dimensions, as it couples all scales in the problem. 
Beylkin~\etal \cite{Beylkin} introduced a different approach, which they called
the non-standard representation, where the scales are explicitly separated. By
organizing the operator as a collection of triples
\begin{equation}
    \T{N}{N} =\ \T{0}{0}\ +\ \sum_{n=0}^{N-1} \big(\ \A{n}{n}\ +\ \B{n}{n}\ +\ \C{n}{n}\ \big) 
\end{equation}
where each triple $(\ \A{n}{n},\ \B{n}{n},\ \C{n}{n})$ corresponds to the interaction
at a particular scale $n$. The interaction \emph{between} different length scales are
not explicitly treated in this representation, and needs to be accounted for in
a post-prosessing step. In order to achieve this separation of scales some redundancy
is necessary in the function representations for $f$ and $g$, as we need to keep the 
scaling projections at \emph{all} scales. The operator matrix that is applied to the 
function will in this case be
\begin{align}
	\small
	\label{eq:NSmatrix}
	\begin{split}
	\left(
	\begin{array}{c|c}
		\begin{array}{c|c}
			$\multirow{4}{*}{\T{N-1}{N-1}}$&
			$\multirow{4}{*}{\C{N-1}{N-1}}$
			\\ &\\ &\\ &\\ \hline
			$\multirow{4}{*}{\B{N-1}{N-1}}$&
			$\multirow{4}{*}{\A{N-1}{N-1}}$
			\\ &\\ &\\ &\\
		\end{array}
		&
		\begin{array}{cc}
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
		\end{array}\\\hline
		\begin{array}{c}
			\begin{array}{cc}
				&\\&
			\end{array}\\
			\begin{array}{cc}
				&\\&
			\end{array}\\
		\end{array}
		&
		\begin{array}{c|c}
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \C{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\\hline
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \B{N-1}{N-1}\ \ \ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \A{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\
		\end{array}
	\end{array}
	\right)	\left(
	\begin{array}{c}
		$\multirow{4}{*}{\scalingrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{4}{*}{\waveletrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\\hline
		$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\end{array}
	\right)	
	%&= \left(
	%\begin{array}{c}
	%	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\\hline
	%	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	%\end{array}
	%\right)
	\end{split}
\end{align}
and although the total matrix has grown in size, this representation leads to 
straightforward adaptive algorithms, as the operator can be applied one scale
at the time, starting from the coarsest (usually $n=0$). As pointed out above, 
this does not directly account for the interaction between scales, but this can
be included by a series of wavelet transforms on parts of the result. This is 
described fully in the implementation part in chapter \ref{chap:implementation}. 
The post-processing wavelet transforms requires $O(N)$ operations, and provided
sparse $A$, $B$ and $C$ parts of the operator, the complete non-standard 
application scales as $O(N)$, in contrast to the standard form, where 
scale-to-scale interactions are treated explicitly, which has a formal 
$O(N log N)$ scaling \cite{something}.

\subsection{Integral operators}
Calderon-Zygmund operators\\
Vanishing moments -> sparsity\\

\section{Separated representation of operators}
In the discussion of multi-dimensional operators in the previous section it
was assumed that we have an operator that is separable in the Cartesian
coordinates. This assumption is necessary to in order to make calculations
feasable in higher dimensions, as the straightforward generalization of a
one-dimensional approach will scale exponentially in the dimension. It is,
however, not necessary that the operator separates exactly, and it has been
shown that a lot of physically interresting operators can be approximated


\subsection{Poisson kernel}
In order to solve the Poisson equation using the methods described above, we
need to rewrite it to an integral form. The equation, in its differential
form, is given as
\begin{equation}
	\nabla^2 V(\bs{x}) = -\rho(\bs{x})
\end{equation}
where $\rho(\bs{x})$ is the known (charge) distribution, and
$V(\bs{x})$ is the unknown (electrostatic) potential. It is a standard
textbook procedure to show that the solution can be written as the integral
\begin{equation}
	V(\bs{x}) = \int G(\bs{x},\bs{y})\rho(\bs{y})d\bs{y}
\end{equation}
where $G(\bs{x},\bs{y})$ is the Green's function which
is the solution to the \emph{fundamental} equation with \emph{homogeneous}
(Dirichlet) boundary conditions
\begin{eqnarray}
	\label{eq:fundamental}
	\begin{split}
	\nabla^2 G(\bs{x},\bs{y}) &= \delta(\bs{x}-\bs{y})&\\
	G(\bs{x},\bs{y}) &= 0 &,\bs{x} \in boundary
	\end{split}
\end{eqnarray}
This equation can be solved analytically and the Green's function is given
(in three dimensions) simply as
\begin{equation}
	\label{eq:3DGreen}
	G(\bs{x},\bs{y}) = \frac{1}{||\bs{x}-\bs{y}||}
\end{equation}
This is the well known potential arising from a point charge located in the 
position $\bs{y}$, which is exactly what eq.(\ref{eq:fundamental})
describes.

\subsection{Helmholtz kernel}

\subsection*{Numerical separation of the kernel}
The Green's function kernel as it is given in eq.(\ref{eq:3DGreen}) is not 
separable in the cartesian coordinates. However, since we are working with
finite precision we can get by with an \emph{approximate} kernel as long as
the error introduced with this approximation is less than our overall accuracy
criterion. If we are able to obtain such a \emph{numerical} separation of the
kernel, the operator can be applied in one direction at the time, allowing us 
to use the expressions derived above for one-dimensional integral operators to 
solve the three-dimensional Poisson equation. This is of great importance, not 
only because we do not have to derive the $d$-dimensional operator equations, 
which is at best notationally awkward, but also because it again reduces the 
scaling behavior to become linear in the dimension of the system.\\

\noindent
The Poisson kernel can be made separable by expanding it as a
sum of Gaussian functions, specifically
\begin{equation}
	\label{eq:poissonexp}
	\frac{1}{||\bs{r}-\bs{s}||} \approx
	\sum_{\kappa=1}^{M_\epsilon}a_\kappa e^{-b_{\kappa}(\bs{r}-\bs{s})^2}
\end{equation}
where $a_\kappa$ and $b_\kappa$ are parameters that needs to be determined,
and the number of terms $M_\epsilon$, called the separation rank, depends on 
the accuracy requirement and on what interval this expansion needs to be
valid. Details of how to obtain this expansion can be found in 
\cite{Fossgaard}, and
will not be treated here, but it should be mentioned that the separation rank
is usually in the order of $100$, e.g. it requires $M_\epsilon = 70$ to
reproduce $1/r$ on the interval $[10^{-5}, \sqrt{3}]$ in three dimensions with
error less than $\epsilon = 10^{-8}$.\\

\subsection{Multiresolution operators in $d$ dimensions}
We assume that we have a separable representation of a $d$-dimensional
operator $\mathcal{T}$ such that
\begin{equation}
    \mathcal{T} = \bigotimes_{i=1}^d T_i
\end{equation}
where $T_p$ correspond to a one-dimensional operator as described above.
As for the one-dimensional case we have the equation
\begin{equation}
    \scalingrep{\hat{g}}{n+1} = \bigotimes^d\ \T{n+1}{n+1} \scalingrep{f}{n+1}
\end{equation}
which we can decompose to
\begin{equation}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\bigotimes^d\ \Big(\ \A{n}{n} +\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) 
	\big(\scalingrep{f}{n} + \waveletrep{f}{n}\big)
\end{equation}
and we can simplify the notation in the following way
\begin{equation}
    \A{n}{n} = O^{11,n} \qquad
    \B{n}{n} = O^{10,n} \qquad
    \C{n}{n} = O^{01,n} \qquad
    \T{n}{n} = O^{00,n}
\end{equation}
and the tensor product of the operator can be written
\begin{equation}
    \bigotimes^d\ \Big(\ \A{n}{n}+\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) =
	\sum_{\alpha=0}^{2^d-1} \sum_{\beta=0}^{2^d-1} O^{\alpha,\beta,n}
\end{equation}
where we define
\begin{equation}
    O^{\alpha\beta,n} \mydef \bigotimes_{i}^d\ O^{\alpha_i\beta_i,n}
\end{equation}
with $0 \leq \alpha < 2^d$ and $0 \leq \beta < 2^d$ and $\alpha_i$ and $\beta_i$
are defined by the binary expansion of $\alpha$ and $\beta$ in $d$ dimensions.
We can now obtain a completely equivalent structure as for the mono-dimensional
case
\begin{equation}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\Big(\mathcal{A}^n + \mathcal{B}^n + \mathcal{C}^n + \mathcal{T}^n\Big)
	\big(\scalingrep{f}{n} + \waveletrep{n}{n}\big)
\end{equation} 
with the following definitions
\begin{equation}
    \mathcal{A}^{n} \mydef 
    \sum_{\alpha=1}^{2^d-1} \sum_{\beta=1}^{2^d-1} O^{\alpha\beta,n}, \quad
    \mathcal{B}^{n} \mydef 
    \sum_{\alpha=1}^{2^d-1} O^{\alpha 0,n}, \quad
    \mathcal{C}^{n} \mydef 
    \sum_{\beta=1}^{2^d-1} O^{0 \beta,n}, \quad 
    \mathcal{T}^{n} \mydef O^{00,n}
\end{equation}
We could now proceed with a further decomposition of the scaling parts of the
operator and functions to the next coarser scale, obtaining the standard
representation of the operator in multiple dimension. However, the notation
then becomes very complicated, and, as we will see in the later in the 
implementation part in section \ref{sec:implementation}, this is not needed
in the so-called non-standard representation of the operator.

