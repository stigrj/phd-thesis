\chapter{Mathematical Theory}
Wavelet theory is a rather young field of mathematics, first appearing in the
late 1980s. The initial application was in signal theory \cite{Strang} but in
the early 90s, wavelet-based methods started to appear for the solution of
PDEs and integral equations \cite{Beylkin90}\cite{Alpert93}, and in recent 
years for application in electronic structure calculations
\cite{Harrison}\cite{Niklasson}\cite{Arias}. The presented work is based on the
generalization called multiwavelets, which was introduced by Alpert\cite{Alpert}.

In this chapter a general introduction to multiwavelet theory will be given through 
the concept of multiresolution analysis (MRA)\footnote{Keinert\cite{Keinert} uses 
the term multiresolution \emph{approximation}, but in this work we will use
multiresolution \emph{analysis}, as it is more commonly used in the literature.}.
A detailed description of MRAs can be found in Keinert\cite{Keinert}, from which a 
brief summary of the key issues are given in the following, with the difference that 
we limit our discussion to the unit interval instead of the real line. This work is 
concerned with orthogonal MRA only, and for a description of the general bi-orthogonal 
MRA the reader is referred to Keinerts book. 

\section{Multiresolution Analysis}
A multiresolution analysis of $L^2([0,1])$ is an infinite nested sequence of subspaces
\begin{equation}
    \label{eq:MRA}
    \scalingspace{0}\subset \scalingspace{1}\subset \cdots \subset 
	\scalingspace{n}\subset \cdots \subset L^2([0,1])
\end{equation}
with the following properties
\begin{enumerate}
    \item $\scalingspace{\infty}\ is\ dense\ in\ L^2([0,1])$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(2x) \in \scalingspace{n+1}
    	\ ,\ \forall n \in \mathbb{N}$.
    \item $f(x) \in \scalingspace{n} \Longleftrightarrow f(x-2^{-n}l) \in \scalingspace{n}
	\ ,\ \forall n \in \mathbb{N},\ 0 \leq l \leq 2^n - 1.$
    \item There exists a function vector $\scalingvec$ of length $k+1$ in 
	$L^2([0,1])$ so that \[\lbrace \scaling_j(x):\  0\leq j\leq k \rbrace\] 
	forms a basis for \scalingspace{0}. 
\end{enumerate}
This means that if we can construct a basis of \scalingspace{0}, which consists of only
$k+1$ functions, we can construct a basis of \emph{any} space \scalingspace{n}, by
simple compression (by a factor of $2^n$), and translations (to all dyadic
grid points at scale $n$), of the original $k+1$ functions, and by increasing
the scale $n$, we are approaching a complete basis of $L^2([0,1])$. Since $\scalingspace{n}
\subset \scalingspace{n+1}$ the basis functions of \scalingspace{n} can be expanded in the 
basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalescaling}
    \scalingvec^n_l(x) \mydef 2^{n/2}\scalingvec(2^nx-l) = 
	\sum_{l'=0}^{2^n-1} H^{(l')} \scalingvec^{n+1}_{l'}(x)
\end{equation}
where the $H^{(l')}$s are the so-called filter matrices that describes the 
transformation between different spaces \scalingspace{n}. The MRA is called orthogonal if 
\begin{equation}
    \label{eq:orthogonality}
    \langle \scalingvec_l^n(x), \scalingvec_{l'}^n(x) \rangle = \delta_{l,l'}I_{k+1}
\end{equation}
where $I_{k+1}$ is the $(k+1) \times (k+1)$ unit matrix, and $k+1$ is the 
length of the function vector. This orthogonality condition means that the 
functions are orthogonal both within one function vector and through all 
possible translations on one scale, but \emph{not} through the different scales.

Complementary to the nested sequence of subspaces \scalingspace{n}, we can define 
another series of spaces \waveletspace{n} that complements \scalingspace{n} in 
\scalingspace{n+1}
\begin{equation}
    \label{eq:MRAcomplement}
    \scalingspace{n+1} = \scalingspace{n} \oplus \waveletspace{n}
\end{equation}
where there exists another function vector $\waveletvec$ of lenght $k+1$ that, with 
all its translations on scale $n$ form a basis for \waveletspace{n}.
Analogously to Eq.~(\ref{eq:twoscalescaling}) the function vector can be expanded 
in the basis of \scalingspace{n+1}
\begin{equation}
    \label{eq:twoscalewavelet}
    \waveletvec^n_l(x) \mydef 2^{n/2}\waveletvec(2^nx-l) = 
	\sum_{l'=0}^{2^n-1} G^{(l')}\scalingvec_{l'}^{n+1}(x)
\end{equation}
with filter matrices $G^{(l')}$. In orthogonal MRA the functions $\waveletvec$ fulfill 
the same othogonality condition as Eq.~(\ref{eq:orthogonality}), and if we combine 
Eq.~(\ref{eq:MRA}) and Eq.~(\ref{eq:MRAcomplement}) we see that they must also be 
orthogonal with respect to different scales
\begin{equation}
    \langle \waveletvec_l^n(x), \waveletvec_{l'}^{n'}(x) \rangle = 
	\delta_{l,l'}\delta_{n,n'}I_{k+1}
\end{equation}
Recursive application of Eq.~(\ref{eq:MRAcomplement}) yields the important relation
\begin{equation}
    \label{eq:MRArecursive}
    \scalingspace{n} = \scalingspace{0} \oplus \waveletspace{0} \oplus \waveletspace{1}
	\oplus \cdots \oplus \waveletspace{n-1}
\end{equation}

\section{Multiwavelets}
There are many ways to choose the basis functions $\scalingvec$ and $\waveletvec$ 
(which define the spanned spaces \scalingspace{n} and \waveletspace{n},
leading to different wavelet families. There is a one-to-one correspondence between 
the basis functions $\scalingvec$ and $\waveletvec$, and the filter matrices $H^{(l)}$ 
and $G^{(l)}$ used in the two-scale relations Eq.~(\ref{eq:twoscalescaling}) and 
Eq.~(\ref{eq:twoscalewavelet}), and most well known wavelet families are defined 
only through their filter coefficients.

In the following we are taking a different approach, which follows the original 
construction of multiwavelets by Alpert\cite{Alpert93}. We define the \emph{scaling 
space} \scalingspace{n} as the space of piecewise polynomials
\begin{equation}
    \begin{array}{rl}
        \displaystyle \scalingspace{n} \mydef & \lbrace f:\ all\ polynomials\ of\ degree\ \leq\ k\\ 
	\displaystyle &\ on\ the\ interval\ (2^{-n}l,2^{-n}(l+1))\\
	\displaystyle &\ for\ 0 \leq l < 2^n,\ f\ vanishes\ elsewhere \rbrace
    \end{array}
\end{equation}
This definition fulfills the conditions for a multiresolution analysis, and if the 
basis is chosen to be orthogonal, the \scalingspace{n} constitutes an \emph{orthogonal} MRA.

\subsection{The scaling basis}
The construction of the scaling functions is quite straightforward; $k+1$ orthogonal 
polynomials are chosen to span the space of polynomials of degree $\leq k$ on the unit 
interval. The total scaling basis for \scalingspace{n} is then obtained by appropriate dilation 
and translation of these functions. One way to construct the basis is to start with the 
standard basis $\lbrace 1,x,x^2,\dots,x^k\rbrace$ and orthonormalize with respect to 
the $L^2$ inner product on the unit interval.

\subsection{The wavelet basis}
The \emph{wavelet space} \waveletspace{n} is defined, according to Eq.~(\ref{eq:MRAcomplement}), 
as the orthogonal complement of \scalingspace{n} in \scalingspace{n+1}. The wavelet basis functions 
of \waveletspace{n} are hence piecewise polynomials of degree $\leq k$ on \emph{each} of the two 
intervals on scale n+1 that overlaps with \emph{one} interval on scale n (but may be 
discontinous in the merging point). In the construction of the wavelet basis these 
piecewise polynomials are made orthogonal to the basis of \scalingspace{n} and to each other, 
and, following Alpert\cite{Alpert93}, a standard Gram-Schmidt orthogonalization is 
employed to construct a basis that meet the necessary orthogonality conditions.

One important property of the wavelet basis is its number of vanishing moments. 
The k-th continuous moment of a function $\wavelet$ is defined as the integral
\begin{equation}
	\mu_k \mydef \int_0^1 x^k\wavelet(x)dx 	
\end{equation}
and the function $\wavelet$ has $M$ vanishing moments if \[\mu_k = 0,\qquad k=0,\dots, M-1 \]
The vanishing moments of the wavelet functions gives information on the
approximation order of the scaling functions. If the wavelet function
$\wavelet$ has $M$ vanishing moments, any polynomial of order $\leq M-1$ can be 
exactly reproduced in the scaling space, and the error in
representing an arbitrary function in the scaling basis is of $M$-th order. 
By construction, $x^i$ is in the space \scalingspace{0} for $0\leq i \leq k$, and since
$\waveletspace{0} \perp \scalingspace{0}$, the first $k+1$ moments of $\wavelet^0_j$ must vanish.

\subsection{Filter relations}
With the multiwavelet basis defined, we can construct the filter
matrices that fulfill the two-scale relations in Eq.(\ref{eq:twoscalescaling}) and
Eq.(\ref{eq:twoscalewavelet}). The details of this construction can be found in
Alpert \etal\cite{Alpert02}, and will not be presented here, but we specifically 
end up with four matrices $H^{(0)}, H^{(1)}, G^{(0)}$ and $G^{(1)}$, whose size and 
content are dependent on the order and type of the chosen scaling functions. 
Eq.~(\ref{eq:twoscalescaling}) and Eq.~(\ref{eq:twoscalewavelet}) thus reduces to
\begin{equation}
    \label{eq:twoscalerelations}
    \begin{pmatrix}
	\wavelet_{l}^{n}\\
	\scaling_{l}^{n}\\
    \end{pmatrix}=
    \begin{pmatrix}
	G^{(1)}&G^{(0)}\\
	H^{(1)}&H^{(0)}\\
    \end{pmatrix}
    \begin{pmatrix}
	\scaling_{2l+1}^{n+1}\\
	\scaling_{2l}^{n+1}\\
    \end{pmatrix}
\end{equation}

\subsection{Multiwavelets in $d$ dimensions}
Multidimensional wavelets are usually constructed by tensor products, where the
scaling space is defined as
\begin{equation}
    \scalingspace{n,d} \mydef \bigotimes^d \scalingspace{n}
\end{equation}
The basis for this $d$-dimensional space is given as tensor products of the
one-dimensional bases.
\begin{equation}
    \label{eq:multidimscaling}
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x}) = 
    \scalingnd^n_{j_1 j_2\dots j_d,l_1 l_2\dots l_d} (x_1,x_2,\dots,x_d) \mydef
    \prod_{i=1}^d \scaling^n_{j_i,l_i}(x_i)
\end{equation}
The number of basis functions on each hypercube $\bs{l}=(l_1,l_2,\dots,l_d)$ 
becomes $(k+1)^d$, while the number of such hypercubes on scale $n$ becomes $2^{dn}$, 
which again means that the total number of basis functions is growing exponentially 
with the number of dimensions.

The wavelet space can be defined using Eq.~(\ref{eq:MRAcomplement})
\begin{equation}
    \label{eq:multidimW}
    \scalingspace{n+1,d} = \bigotimes^d \scalingspace{n+1} = 
	\bigotimes^d (\scalingspace{n} \oplus \waveletspace{n})
\end{equation}
where the pure scaling term obtained when expanding the product on the right
hand side of Eq.~(\ref{eq:multidimW}) is recognized as \scalingspace{n,d}, making the
wavelet space \waveletspace{n,d} consist of all the remaining terms of the product, 
which are terms that contain at least one wavelet space.

To achieve a uniform notation, we can introduce a "generalized" one-dimensional
wavelet function $\lbrace\scalewave_{j,l}^{\alpha,n}\rbrace$ that, depending on 
the index $\alpha$ can be either the scaling or the wavelet function
\begin{equation}
    \scalewave^{\alpha_i,n}_{j_i,l_i} \mydef 
    \left\{
	\begin{array}{lll}
	    \scaling^n_{j_i,l_i}	&\mbox{ if }\alpha_i = 0\\
	    \wavelet^n_{j_i,l_i}	&\mbox{ if }\alpha_i = 1
	\end{array}
    \right.
\end{equation}
The wavelet functions for the $d$-dimensional space can thus be expressed as
\begin{equation}
    \waveletnd^{\alpha,n}_{\bs{j}, \bs{l}}(\bs{x}) =
    \prod_{i=1}^d\scalewave^{\alpha_i,n}_{j_i,l_i}(x_i)
\end{equation}
Where the total $\alpha$ index on $\waveletnd$ separates the $2^d$ different
possibilities of combining scaling/wavelet functions with the same index
combination $\bs{j} = (j_0,j_1,\dots,j_k)$. $\alpha$ is given by the 
binary expansion
\begin{equation}
    \alpha = \sum_{i=1}^d 2^{i-1}\alpha_i
\end{equation}
and thus runs from $0$ to $2^d-1$. By closer inspection we see that $\alpha=0$
recovers the pure scaling function
\begin{equation}
    \waveletnd^{0,n}_{\bs{j},\bs{l}}(\bs{x}) \equiv
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
and we will keep the notation $\scalingnd^n_{\bs{j},\bs{l}}$ for the
scaling function, and exclude the $\alpha=0$ term in the wavelet notation
when treating multidimensional functions.

We can immediately see that the dimensionality of the wavelet space is higher
than the scaling space on the same scale $n$, specifically $2^d-1$ times
higher. This must be the case in order to conserve the 
dimensionality through the equation
\begin{equation}
    \scalingspace{n+1,d} = \scalingspace{n,d} \oplus \waveletspace{n,d}
\end{equation}
since $dim(\scalingspace{n+1,d}) = 2^d dim(\scalingspace{n,d})$.

As for the monodimensional case we can define filter matrices that transform
the scaling functions at scale $n+1$,
$\lbrace\scalingnd^{n+1}_{\bs{j},\bs{l}}\rbrace$, into scaling and 
wavelet functions at scale $n$, $\lbrace\waveletnd^{\alpha,n}_{\bs{j},
\bs{l}}\rbrace_{\alpha=0}^{2^d-1}$. Details of this construction can be
found in \cite{Fossgaard}, where the corresponding matrices are shown to be 
tensor products of the monodimensional matrices. 

\section{Function representation}
In this section we will describe how to represent functions in the multiwavelet
basis, as well as how to perform simple arithmetic operations.

\subsection{Function projection}
We introduce the projection operator \scalingproj{n} onto the basis 
$\lbrace\scaling^n_{j,l}\rbrace$ that span the scaling space \scalingspace{n}
\begin{equation}
    \label{eq:scaling_exp}
    f(x) \approx \scalingproj{n}f(x) \mydef \scalingrep{f}{n}(x) =
	\sum_{l=0}^{2^n-1}\sum_{j=0}^k\scalingcoef^{n,f}_{j,l}\scaling^n_{j,l}(x)
\end{equation}
where the expansion coefficients $\scalingcoef^{n,f}_{j,l}$, the so-called \emph{scaling}
coefficients, are obtained by the projection integral
\begin{equation}
    \label{eq:scaling_coef}
    \scalingcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\scaling^n_{j,l}(x)\ud x
\end{equation}
The accuracy of this approximation is determined by the scale $n$ at which the
projection is performed: the higher the scale, the better the approximation.

\subsection{Multiresolution functions}
We can also introduce the projection operator \waveletproj{n} that projects
onto the wavelet basis $\lbrace\wavelet^n_{j,l}\rbrace$ of the space \waveletspace{n}
\begin{equation}
    \label{eq:wavelet_exp}
    \waveletproj{n}f(x) \mydef \waveletrep{f}{n}(x) =
    \sum_{l=0}^{2^n-1}\sum_{j=0}^k\waveletcoef^{n,f}_{j,l}\wavelet^n_{j,l}(x)
\end{equation}
where the \emph{wavelet} coefficients are given as
\begin{equation}
    \label{eq:wavelet_coef}
    \waveletcoef^{n,f}_{j,l} \mydef \int_0^1f(x)\wavelet^n_{j,l}(x)\ud x
\end{equation}
According to Eq.~(\ref{eq:MRAcomplement}) we have the following relationship 
between the projection operators
\begin{equation}
    \label{eq:proj_rel}
    \scalingproj{n+1} = \scalingproj{n} + \waveletproj{n}
\end{equation}
which means that the wavelet projection should not be regarded as an approximation 
of the function $f$, but rather the difference between two approximations
\begin{equation}
    \waveletrep{f}{n} = \waveletproj{n} f = (\scalingproj{n+1} - \scalingproj{n}) f = 
	\scalingrep{f}{n+1} - \scalingrep{f}{n}
\end{equation}
This means that the wavelet projection \waveletrep{f}{n} can be used as a measure of the accuracy 
of the scaling projection \scalingrep{f}{n}, provided that the projection sequence is converging,
$\lim_{n\rightarrow\infty} \scalingrep{f}{n} = f$, which is the case for square integrable functions.
A given approximation $f^N$ can then be expressed as the much coarser approximation \scalingrep{f}{0}
with a number of wavelet corrections
\begin{align}
    \label{eq:highres}
    f(x)    &\approx \scalingrep{f}{N}(x)\\
    \label{eq:multires}
	    &= \scalingrep{f}{0}(x) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(x)
\end{align}
These equivalent representations are the high-resolution and multi-resolution 
approximations, respectively, of the function $f$. The filter matrices $H^{(0)}, 
H^{(1)}, G^{(0)}$ and $G^{(1)}$ of Eq.~(\ref{eq:twoscalerelations}) allow us to 
change between the representations of Eq.~(\ref{eq:highres}) and 
Eq.~(\ref{eq:multires}), and the disjoint support of the basis functions ensures 
that this is a local transformation.

\subsection{Multiresolution functions in $d$ dimensions}
The multi-dimensional function representation is obtained similarly to
Eq.~(\ref{eq:scaling_exp}) by projection onto the multi-dimensional basis
Eq.~(\ref{eq:multidimscaling})
\begin{equation}
    \label{eq:scaling_exp_nd}
    f(\bs{x}) \approx \scalingrep{f}{n}(\bs{x}) = \sum_{\bs{l}}
    \sum_{\bs{j}} \scalingcoef^{n,f}_{\bs{j},\bs{l}} 
    \scalingnd^n_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the sums are over all possible translation vectors 
$\bs{l} = (l_1,\dots,l_d)$ for $0\leq l_i\leq 2^n-1$, and all possible 
scaling function combinations $\bs{j} = (j_1,\dots,j_d)$ for 
$0\leq j_i\leq k$. The scaling coefficients are obtained by the
multi-dimensional integral
\begin{equation}
    \label{eq:waveletproj_nd}
    \scalingcoef^{n,f}_{\bs{j},\bs{l}} \mydef
    \int_{[0,1]^d}f(\bs{x})\scalingnd^n_{\bs{j},
    \bs{l}}(\bs{x})\ud \bs{x}
\end{equation}
The wavelet components are given as
\begin{equation}
    \waveletrep{f}{n}(\bs{x}) = \sum_{\bs{l}} \sum_{\bs{j}} 
    \sum_{\alpha=1}^{2^d-1}\waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} 
	\waveletnd^{\alpha,n}_{\bs{j},\bs{l}}(\bs{x})
\end{equation}
where the $\bs{l}$ and $\bs{j}$ summations are the same as in
Eq.~(\ref{eq:scaling_exp_nd}), and the $\alpha$ sum is over all combinations of
scaling/wavelet functions (excluding the pure scaling $\alpha=0$).
The expansion coefficients are obtained by the multi-dimensional projection
\begin{equation}
    \waveletcoef^{\alpha,n,f}_{\bs{j},\bs{l}} \mydef
	\int_{[0,1]^d}f(\bs{x})\waveletnd^{\alpha,n}_{\bs{j},
	\bs{l}}(\bs{x})d\bs{x}
\end{equation}
We can again approximate the function $f(\bs{x})$ at scale $N$ and 
decompose it into its multiresolution components
\begin{equation}
    f(\bs{x}) \approx \scalingrep{f}{N}(\bs{x}) = 
    \scalingrep{f}{0}(\bs{x}) + \sum_{n=0}^{N-1} \waveletrep{f}{n}(\bs{x})
\end{equation}
The $d$-dimensional filter matrices are obtained by tensor products of the
mono-dimensional filters. This means that by the tensor structure of
the multi-dimensional basis, we can perform the wavelet transform one dimension 
at the time, and the work done scales linearly in the dimension. If the full
$d$-dimensional filter matrix had been applied, the work would have scaled
as the power of the dimension, hence limiting the practical use in higher
dimensions. A more rigorous treatment of the multi-dimensional wavelet
transforms can be found in Tymczak\cite{Tymczak}.

\subsection{Addition of functions}
The addition of functions in the multiwavelet basis is quite straightforward, 
since it is represented by the mappings
\begin{align}
    \label{eq:addmap}
    \begin{split}
	\scalingspace{n} + \scalingspace{n} &\rightarrow \scalingspace{n}\\
	\waveletspace{n} + \waveletspace{n} &\rightarrow \waveletspace{n}
    \end{split}
\end{align}
This basically means that the projection of the sum equals the sum of the
projections. In the polynomial basis this is simply the fact that the sum of
two $k$-order polynomials is still a $k$-order polynomial.

\subsection{Multiplication of functions}
Multiplication of functions in the multiwavelet basis is somewhat more
involved than addition. The reason for this is that, in contrast to
Eq.~(\ref{eq:addmap}), the product is represented by the mapping \cite{Beylkin92}
\begin{equation}
    \label{eq:multmap}
    \scalingspace{n} \times \scalingspace{n} \rightarrow V^n_{2k}
\end{equation}
This means that the product of two functions falls outside of the MRA and needs 
to be projected back onto the scaling space sequence. In particular this means that 
the product of two functions on a given scale ''spills over'' into the finer scales
\begin{equation}
    \label{eq:multmapinf}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{\infty} \waveletspace{n'}
\end{equation}
Working with a finite precision it is desirable to make the product as accurate
as each of the multiplicands. This is done by terminating the sum in
Eq.~(\ref{eq:multmapinf}) at some sufficiently large scale $N$
\begin{align}
    \label{eq:multmapN}
    \scalingspace{n} \times \scalingspace{n} \rightarrow \scalingspace{n} \oplus 
	\bigoplus_{n'=n}^{N-1} \waveletspace{n'} &= \scalingspace{N}
\end{align}
Assume now that $n$ is the finest scale present in either of the multiplicands,
and $N>n$ is the finest scale present in the product. An algorithm to determine 
the maximum scale $N$ needed in the result will be presented in section 
\ref{sec:implementation}, and in the following it is simply assumed that $N$ is 
known a priori. We know that
\begin{equation}
    \scalingspace{n} \subset \scalingspace{n+1} \subset \cdots \subset \scalingspace{N}
\end{equation}
which means that the multiplication could just as well have been written 
\begin{equation}
    \scalingspace{N} \times \scalingspace{N} \rightarrow \scalingspace{N}
\end{equation}
where the representations of the multiplicands on scale $N$ is obtained by a
series of backward wavelet transforms. This will result in an \emph{oversampling} 
of the multiplicands which allows us to relate the scaling projections of the product
on scale $N$ to the projections of the multiplicands on the same scale.

Finally, when we have obtained the scaling projection of the product on scale
$N$ we do a wavelet decomposition to obtain wavelet projections on 
the coarser scales. 

\section{Operator representation}
In this section we discuss the multiresolution analysis of a general operator $T$
\begin{equation}
    g(x) = [Tf](x)
\end{equation}
and we describe two different multiresolution representation of the operator: the 
so-called standard and non-standard representations. The difference between 
the two is largely a matter of implementation, as they are mathematecally equivalent,
but as we will see below, the non-standard form leads to considerably simpler algorithms, 
especially in the multi-dimensional implementation. In the standard representation the
operator couples all length scales in all dimensions, leading to a very complicated
operator structure, while in the non-standard representation the different scales are 
decoupled in the operator application, while the interaction between scales are handled 
by a post-processing step.

An essential feature in the discussion of operators in the multiresolution framework
is the number of vanishing moments of the chosen basis. This property leads to 
effectively sparse representations of certain operators (in the sense that sparse
representations can be obtained to a given accuracy by a priori thresholding of small 
coefficients), and fast (linear-scaling) algorithms can be obtained for the operator 
application.

A necessary assumption for an efficient implementation of a multi-dimensional operator
is that it is separable in the Cartesian coordinates. This, combined with the tensor
structure of the multiwavelet basis, ensures that the multi-dimensional operator 
application can be performed using mono-dimensional algorithms, and that the exponential
scaling in the dimension is significantly reducecd. This assumption does not
limit the applicability of the method on real-world problems, as many important 
non-separable operators in physics can be made separable to a finite, but arbitrary
precision.

\subsection{Operator projection}
Working in the multiresolution analysis, the operator is applied to the projection 
of $f$ at a given scaling space \scalingspace{n}
\begin{equation}
    \hat{g}(x) = [T\scalingproj{n}f](x) 
\end{equation}
and we are looking for the projected solution
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}f](x) 
\end{equation}
Using the fundamental property of projection operators 
$\scalingproj{n}\scalingproj{n} = \scalingproj{n}$ we get
\begin{equation}
    \scalingproj{n}\hat{g}(x) = [\PTP{n}{n}\scalingproj{n}f](x) 
\end{equation}
and we can represent the full operator application on scale $n$
\begin{equation}
    \scalingrep{\hat{g}}{n} (x) =\ \T{n}{n}\scalingrep{f}{n} (x)
\end{equation}
where the projection of the operator $T$ at the scaling space \scalingspace{n}
is defined as
\begin{equation}
    \label{eq:defTn}
    \T{n}{n} \mydef \scalingproj{n} T \scalingproj{n}
\end{equation}
This operation should be performed at a scale $N$ where the overall accuracy of
the representations are satisfactory, and we can assume that
\begin{equation}
    \scalingrep{\hat{g}}{N} \approx \scalingrep{g}{N} \mydef (Tf)^N \approx g
\end{equation}
Algorithms for how to achieve this accuracy is presented in chapter 
\ref{chap:implementation}.

\subsection{Multiresolution operators}
Making use of Eqs.~(\ref{eq:defTn}) and (\ref{eq:proj_rel}) we can decompose the 
scaling representation of the operator at scale $n+1$ into scaling and wavelet contributions
at the next coarser scale
\begin{align}
    T	&\approx    \PTP{n+1}{n+1}\\
	&=	    \big(\scalingproj{n} + \waveletproj{n}\big) T 
		    \big(\scalingproj{n} + \waveletproj{n}\big)\\
	&=	    \PTP{n}{n} +\ \PTQ{n}{n} +\ \QTP{n}{n} +\ \QTQ{n}{n}
\end{align}
and we simplify the notation with the following definitions, including a generalization of the
definition in Eq.~(\ref{eq:defTn})
\begin{eqnarray}
    \label{eq:defABCT}
    \begin{split}
    	\A{n}{n'} &\mydef& \QTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \waveletspace{n}\\
	\B{n}{n'} &\mydef& \QTP{n}{n'}&:& \scalingspace{n'}\rightarrow \waveletspace{n}\\
	\C{n}{n'} &\mydef& \PTQ{n}{n'}&:& \waveletspace{n'}\rightarrow \scalingspace{n}\\
	\T{n}{n'} &\mydef& \PTP{n}{n'}&:& \scalingspace{n'}\rightarrow \scalingspace{n}
    \end{split}
\end{eqnarray}
leading to the relation
\begin{equation}
    \label{eq:ABCT}
    \T{n+1}{n+1} =\ \T{n}{n}\ +\ \C{n}{n}\ +\ \B{n}{n}\ +\ \A{n}{n}\ 
\end{equation} 
The motivation for such a decomposition of the operator lies in the vanishing moments
property of the basis. The $A$, $B$ and $C$ parts of the operator involves projections
onto the wavelet basis, which has the property of vanishing moments, and it has been 
shown \cite{Beylkin} that this leads to sparse representations of certain operators.
This will be discussed in more detail in section \ref{sec:int_oper} when the specific 
operators used in this work are treated.

The decomposition in Eq.~(\ref{eq:ABCT}) can be continued recursively, and by this introduce
more sparsity into the operator, and there are two ways to proceed in order to achieve
this. In the following both the standard and the non-standard form of the multiresolution
operator will be presented.

\subsection{Standard representation}
The standard representation is the straightforward matrix realization of the operator in the 
multiresolution basis. In order to obtain this representation we start with the matrix 
representation in the scaling basis at scale $N$
\begin{equation}
\begin{split} 
    \label{eq:Tmatrix}
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$
	\ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \T{N}{N}
	\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $}$
	\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right) 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ f^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    = 
    \left(
    \begin{array}{c}
	$\multirow{12}{*}{$\ \ g^N\ \ $}$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
This matrix can be decomposed into four submatrices according to Eq.~(\ref{eq:ABCT})
while the functions are decomposed into scaling and wavelet contributions at scale $N-1$
\begin{align}
    \scalingrep{f}{N} &= \scalingrep{f}{N-1} + \waveletrep{f}{N-1}\\
    \scalingrep{g}{N} &= \scalingrep{g}{N-1} + \waveletrep{g}{N-1}
\end{align}
According to Eq.~(\ref{eq:defABCT}) \T{n}{n} and \C{n}{n} produce the scaling part of $g$, 
acting on the scaling and wavelet parts of $f$, respectively. Similarly, \A{n}{n} and 
\B{n}{n} produce the wavelet part of $g$, by acting on the wavelet and scaling parts of $f$, 
respectively. The matrix equation Eq.~(\ref{eq:Tmatrix}) can thus be decomposed as
\begin{equation}
\begin{split}
    \label{eq:ABCTmatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \T{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \C{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \ \B{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{ccc}
	    $\multirow{6}{*}{$\ \ \ \ \ \ \A{N-1}{N-1}\ \ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split} 
\end{equation}
where the size of the total matrix is unchanged. We can now do the same
decomposition of \T{N-1}{N-1} into submatrices at scale $N-2$. The
function components \scalingrep{f}{N-1} and \scalingrep{g}{N-1} need to be 
decomposed as well, so to keep everything consistent, the \B{N-1}{N-1} and 
\C{N-1}{N-1} parts of the operator will have to be transformed accoringly. 
To proceed from here we need the following relations
\begin{align}
    \nonumber
    \B{n}{n}	&= \QTP{n}{n}\\
    \nonumber
		&= \waveletproj{n}T(\scalingproj{n-1}+\waveletproj{n-1})\\
    \nonumber
		&= \QTP{n}{n-1} + \QTQ{n}{n-1}\\
    \label{eq:Bdecomp}
		&=\ \B{n}{n-1} +\ \A{n}{n-1}
\end{align}
and similarly for the $C$ block
\begin{equation}
    \label{eq:Cdecomp}
    \C{n}{n} =\ \C{n-1}{n} +\ \A{n-1}{n}
\end{equation}
which is the change in the operator that is taking place when we decompose 
\scalingrep{f}{n} into $\scalingrep{f}{n-1} + \waveletrep{f}{n-1}$ and \scalingrep{g}{n} 
into $\scalingrep{g}{n-1} + \waveletrep{g}{n-1}$. The matrix equation now turns into
\begin{equation}
\begin{split} 
\label{eq:Smatrix}
    \left(
    \begin{array}{c|c}
	\begin{array}{c|c}
	    $\multirow{3}{*}{\T{N-2}{N-2}}$&
	    $\multirow{3}{*}{\C{N-2}{N-2}}$\\ &\\ &\\
	    \hline
	    $\multirow{3}{*}{\B{N-2}{N-2}}$&
	    $\multirow{3}{*}{\A{N-2}{N-2}}$\\ &\\ &\\
	\end{array} &
	\begin{array}{c}
	    $\multirow{3}{*}{$\ \ \ \ \C{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	    \hline
	    $\multirow{3}{*}{$\ \ \ \ \A{N-2}{N-1}\ \ \ \ $}$
	    \\ \\ \\
	\end{array}\\
	\hline
	\begin{array}{c|c}
	    $\multirow{6}{*}{\B{N-1}{N-2}}$&
	    $\multirow{6}{*}{\A{N-1}{N-2}}$
	    \\ \\ \\ \\ \\ \\
	\end{array} &
	\begin{array}{c}
	    $\multirow{6}{*}{$\ \ \ \ \ \A{N-1}{N-1}\ \ \ \ $}$
	    \\ \\ \\ \\ \\ \\
	\end{array}\\
    \end{array}
    \right)
    \left(
    \begin{array}{c}
	$\multirow{3}{*}{\scalingrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{f}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
    	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\
	\hline
	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
    \end{array}
    \right)
\end{split}
\end{equation}
We can continue this transformation recursively until we reach the coarsest scale. 
Symbolically, we can do the decomposition of Eq.~(\ref{eq:ABCT}) by recursive 
application of itself as well as Eqs.~(\ref{eq:Bdecomp}) and (\ref{eq:Cdecomp}), 
where we gradually introduce more $A$-character into the operator
\begin{eqnarray}
    \nonumber
    \T{N}{N}	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\ \C{n}{n} + 
		    \sum_{n=0}^{N-1}\ \B{n}{n} + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \nonumber
	    	&=\ \T{0}{0} + 
		&   \sum_{n=0}^{N-1}\Big(\ \C{0}{n} + 
		    \sum_{n'<n}\ \A{n'}{n}\Big) +\\
    \nonumber
		&&  \sum_{n=0}^{N-1}\Big(\ \B{n}{0} + 
		    \sum_{n'>n}\ \A{n'}{n}\Big) + 
		    \sum_{n=0}^{N-1}\ \A{n}{n}\\
    \label{eq:MRoperS}
		&=\ ^0T^0 + 
		&   \sum_{n=0}^{N-1}\ \Big(\ \C{0}{n} +\  \B{n}{0} +
		    \sum_{n'=0}^{N-1}\ \A{n}{n'} \Big)
\end{eqnarray}
This multiresolution matrix representation of the operator is called the Standard 
representation\cite{somebeylkin}.

\subsection{Non-Standard representation}
While the standard form of the operator given in Eq.~(\ref{eq:MRoperS}) does
lead to sparse representations, it gives rise to rather complicated algorithms,
especially in several dimensions, as it couples all scales in the problem. 
Beylkin~\etal \cite{Beylkin} introduced a different approach, which they called
the non-standard representation, where the scales are explicitly separated. By
organizing the operator as a collection of triples
\begin{equation}
    \T{N}{N} =\ \T{0}{0}\ +\ \sum_{n=0}^{N-1} \big(\ \A{n}{n}\ +\ \B{n}{n}\ +\ \C{n}{n}\ \big) 
\end{equation}
where each triple $(\ \A{n}{n},\ \B{n}{n},\ \C{n}{n})$ corresponds to the interaction
at a particular scale $n$. The interaction \emph{between} different length scales are
not explicitly treated in this representation, and needs to be accounted for in
a post-prosessing step. In order to achieve this separation of scales some redundancy
is necessary in the function representations for $f$ and $g$, as we need to keep the 
scaling projections at \emph{all} scales. The operator matrix that is applied to the 
function will in this case be
\begin{align}
	\small
	\label{eq:NSmatrix}
	\begin{split}
	\left(
	\begin{array}{c|c}
		\begin{array}{c|c}
			$\multirow{4}{*}{\T{N-1}{N-1}}$&
			$\multirow{4}{*}{\C{N-1}{N-1}}$
			\\ &\\ &\\ &\\ \hline
			$\multirow{4}{*}{\B{N-1}{N-1}}$&
			$\multirow{4}{*}{\A{N-1}{N-1}}$
			\\ &\\ &\\ &\\
		\end{array}
		&
		\begin{array}{cc}
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
			\begin{array}{cc}
				&
			\end{array}
			&
			\begin{array}{cc}
				&
			\end{array}\\
		\end{array}\\\hline
		\begin{array}{c}
			\begin{array}{cc}
				&\\&
			\end{array}\\
			\begin{array}{cc}
				&\\&
			\end{array}\\
		\end{array}
		&
		\begin{array}{c|c}
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \C{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\\hline
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \B{N-1}{N-1}\ \ \ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}
			&
			\begin{array}{ccc}
				\multicolumn{3}{c}{$\multirow{6}{*}
				{\ \ \ \A{N-1}{N-1}\ \ }$}
				\\&&\\&&\\&&\\&\\&\\
			\end{array}\\
		\end{array}
	\end{array}
	\right)	\left(
	\begin{array}{c}
		$\multirow{4}{*}{\scalingrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{4}{*}{\waveletrep{f}{N-2}}$\\ \\ \\ \\\hline
		$\multirow{6}{*}{\scalingrep{f}{N-1}}$\\ \\ \\ \\ \\ \\\hline
		$\multirow{6}{*}{\waveletrep{f}{N-1}}$\\ \\ \\ \\ \\ \\
	\end{array}
	\right)	
	%&= \left(
	%\begin{array}{c}
	%	$\multirow{3}{*}{\scalingrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{3}{*}{\waveletrep{g}{N-2}}$\\ \\ \\\hline
	%	$\multirow{6}{*}{\scalingrep{g}{N-1}}$\\ \\ \\ \\ \\ \\\hline
	%	$\multirow{6}{*}{\waveletrep{g}{N-1}}$\\ \\ \\ \\ \\ \\
	%\end{array}
	%\right)
	\end{split}
\end{align}
and although the total matrix has grown in size, this representation leads to 
straightforward adaptive algorithms, as the operator can be applied one scale
at the time, starting from the coarsest (usually $n=0$). As pointed out above, 
this does not directly account for the interaction between scales, but this can
be included by a series of wavelet transforms on parts of the result. This is 
described fully in the implementation part in chapter \ref{chap:implementation}. 
The post-processing wavelet transforms require $O(N)$ operations, and provided
sparse $A$, $B$ and $C$ parts of the operator, the complete non-standard 
application scales as $O(N)$, in contrast to the standard form, where 
scale-to-scale interactions are treated explicitly, which has a formal 
$O(N log N)$ scaling \cite{something}.

\subsection{Integral operators}
Multiwavelets were originally introduced for their effectively sparse 
representation of certain integral operators, in particular operators with 
non-oscillatory kernels that are analytic except along a finite set of 
curves \cite{Alpert}. To be more specific, we consider one-dimensional operators
on the form
\begin{equation}
    \label{eq:intop1d}
    [Tf](x) = \int K(x,y) f(y) \ud y
\end{equation}
The sparsity of the operator representation follows under certain conditions 
on the integral kernel $K$, which is discussed below. We start, however, by 
expanding the kernel in the multiwavelet basis
\begin{equation}
    \label{eq:kernelexp}
    K^N(x,y) = \sum_{l_x,l_y}\bs{\tau}^{N,N}_{l_xl_y} 
	\scalingvec^N(x)\scalingvec^N(y)
\end{equation}
where the expansion coefficients are given by the integrals
\begin{equation}
    \label{eq:taudef}
    \bs{\tau}^{n_x,n_y}_{l_xl_y} = \int\int
	K(x,y)\scalingvec^{n_x}_{l_x}(x)\scalingvec^{n_y}_{l_y}(y)\ud x \ud y
\end{equation}
Inserting Eq.~(\ref{eq:kernelexp}) into Eq.~(\ref{eq:intop1d}) yields
\begin{align}
    \nonumber
    \T{N}{N}\scalingrep{f}{N}(x) &= \int\left(\sum_{l_x,l_y} \bs{\tau}^{N,N}_{l_xl_y}
	\scalingvec^N_{l_x}(x)\scalingvec^N_{l_y}(y)\right)f(y)\ud y\\
	&= \sum_{l_x,l_y} \bs{\tau}^{N,N}_{l_xl_y}
	\scalingvec^N_{l_x}(x)\int f(y)\scalingvec^N_{l_y}(y)\ud y
\end{align}
where the last integral is recognized as the scaling coefficients of $f$ from 
Eq.~(\ref{eq:scaling_coef})
\begin{equation}
    \label{eq:Tmatrixsum}
    \T{N}{N}\scalingrep{f}{N}(x) = \sum_{l_x,l_y} \bs{\tau}^{N,N}_{l_xl_y}
	\scalingvec^N_{l_x}(x)\scalingcoef^{N,f}_{l_y}
\end{equation}
We can now identify $\bs{\tau}^{N,N}_{l_xl_y}$ as the matrix elements of \T{N}{N} 
and Eq.~(\ref{eq:Tmatrixsum}) is Eq.~(\ref{eq:Tmatrix}) written explicitly. 
Similarly, we define $\bs{\alpha}$, $\bs{\beta}$ and $\bs{\gamma}$ as the matrix 
elements of $A$, $B$ and $C$, respectively
\begin{align}
	\label{eq:tcbadef}
	\begin{split}
	\bs{\alpha}^{n_x,n_y}_{l_xl_y} &= \int\int
	K(x,y)\waveletvec^{n_x}_{l_x}(x)\waveletvec^{n_y}_{l_y}(y)\ud x \ud y\\
	\bs{\beta}^{n_x,n_y}_{l_xl_y} &= \int\int
	K(x,y)\waveletvec^{n_x}_{l_x}(x)\scalingvec^{n_y}_{l_y}(y)\ud x \ud y\\
	\bs{\gamma}^{n_x,n_y}_{l_xl_y} &= \int\int
	K(x,y)\scalingvec^{n_x}_{l_x}(x)\waveletvec^{n_y}_{l_y}(y)\ud x \ud y
	\end{split}
\end{align}
and Eq.~(\ref{eq:Tmatrixsum}) can then be decomposed according to Eq.~(\ref{eq:ABCT}) as
\begin{align}
	\label{eq:ABCTmatrixsum}
	\begin{split}
	[Tf]^N(x) &= \sum_{l_x,l_y}\bs{\tau}^{N-1,N-1}_{l_xl_y}
	\scalingvec^{N-1}_{l_x}(x) \bs{\scalingcoef}^{N-1}_{l_y}\\
	&+ \sum_{l_x,l_y}\bs{\gamma}^{N-1,N-1}_{l_xl_y}
	\scalingvec^{N-1}_{l_x}(x) \bs{\waveletcoef}^{N-1}_{l_y}\\
	&+ \sum_{l_x,l_y}\bs{\beta}^{N-1,N-1}_{l_xl_y}
	\waveletvec^{N-1}_{l_x}(x) \bs{\scalingcoef}^{N-1}_{l_y}\\
	&+ \sum_{l_x,l_y}\bs{\alpha}^{N-1,N-1}_{l_xl_y}
	\waveletvec^{N-1}_{l_x}(x) \bs{\waveletcoef}^{N-1}_{l_y}
	\end{split}
\end{align}
which is Eq.~(\ref{eq:ABCTmatrix}) written explicitly. 

Suppose that the integral kernel satisfy the estimates
\begin{align}
    |K(x,y)| &\leq \frac{1}{|x-y|}\\
    |\partial_x^M K(x,y)|+ |\partial_y^M K(x,y)| &\leq \frac{C_M}{|x-y|^{M+1}}
\end{align}
for some $M\geq1$. Such operators are called Calderon-Zygmund operators, and
include both the Poisson and bound-state Helmholtz operators which is discussed 
in detail below. Beylkin \etal\cite{Beylkin} shows that in a basis with
$M$ vanishing moments, the coefficients $\bs{\alpha}$, $\bs{\beta}$ and 
$\bs{\gamma}$ are bounded as
\begin{equation}
    |\bs{\alpha}_{l_x,l_y}| + |\bs{\beta}_{l_x,l_y}| + |\bs{\gamma}_{l_x,l_y}| 
	\leq \frac{C_M}{1+|l_x-l_y|^{M+1}}
\end{equation}
for all
\begin{equation}
    |l_x-l_y| \geq 2M
\end{equation}
This means that within a given accuracy, all contributions beyond a certain spatial 
separation $|l_x-l_y|$ can be set to zero, leading to operators that are banded 
along the diagonal.

\subsection{Derivative operator}\label{sec:diff_oper}
Alpert \etal described in \cite{Alpert} how to construct derivative operators in
the multiwavelet basis. Since the basis is discontinous, there does not exist a
unique representation of the derivative operator. In \cite{Alpert} this non-uniqueness
appears as two adjustable parameters that handles boundary conditions at the
discontinous merging point between basis functions. The representation can be
viewed as the straightforward differentiation of the basis functions at the 
\emph{interior} of each interval, combined with a finite difference representation
\emph{across} intervals.

The matrix representation of the operator $T=d/dx$ is formally given as
\begin{equation}
    \bs{\tau}_{lm}^{n,n} = \int_{2^{-n}l}^{2^{-n}(l+1)} \scalingvec_{l}^n(x) T \scalingvec_m^n(x) \ud x
	= 2^n \int_0^1 \scalingvec_0^0(x) T \scalingvec_{l-m}^0(x) \ud x
\end{equation}
however, for derivative operators, this integral is not absolutely convergent. 
Because of the disjoint support of the basis functions, it is immidiately clear that
there will be no interaction beyond the neighboring interval, and $\tau_{lm} = 0$ for
$|l-m| > 1$. The case $|l-m| = 1$ needs to be treated with care, since there are
boundary effects to consider even if the basis functions are non-overlapping. This
becomes apparent if we look at the scaling coefficients of the derivative \scalingrep{f'}{n} of
a function \scalingrep{f}{n} represented in the scaling basis at scale $n$
\begin{equation}
    \bs{\scalingcoef}_{l}^{n,f'} = \int_{2^{-n}l}^{2^{-n}(l+1)} 
	\scalingvec_l^n(x) \frac{\ud}{\ud x} \scalingrep{f}{n}(x) \ud x 
\end{equation}
Integration by parts now introduces a boundary term
\begin{align}
    \bs{\scalingcoef}_{l}^{n,f'} 
    &= \scalingrep{f}{n}(x)\scalingvec_l^n(x)|_{2^{-n}}^{2^{-n}(l+1)}
	- \int_{2^{-n}l}^{2^{-n}(l+1)} \scalingrep{f}{n}(x) \frac{\ud}{\ud x} \scalingvec_l^n(x) \ud x \\
    \label{eq:diff_boundary}
    &= 2^{n/2} [\scalingrep{f}{n}(2^{-n}l)\scalingvec_0^0(1) - 
	\scalingrep{f}{n}(2^{-n}(l+1))\scalingvec_0^0(0)] - 2^n\sum_{j=0}^{k} K_{ij}\bs{\scalingcoef}_l^n
\end{align}
where the matrix $K$ is defined
\begin{equation}
    K = \int_0^1 \scalingvec(x) \frac{\ud}{\ud x} \scalingvec^T(x) \ud x
\end{equation}
We see in Eq.~(\ref{eq:diff_boundary}) that the function representation \scalingrep{f}{n} 
needs to be evaluated precisely at the discontinuities of the
basis where the function value is not well defined. This problem is circumvented
by interpolating between the function values obtained at both sides of the boundary
\begin{align}
    \scalingrep{f}{n} &= a\scalingrep{f}{n}_{-} + (1-a) \scalingrep{f}{n}_{+}\\
    \scalingrep{f}{n} &= b\scalingrep{f}{n}_{-} + (1-b) \scalingrep{f}{n}_{+}
\end{align}
where $a$ and $b$ are adjustable parameters. In the Haar basis (piecewise constants)
this reduces to a finite difference definition of the derivative, with the choice 
$a=b=1/2$ corresponding to central difference, and $a=1,b=0$ and $a=0,b=1$ 
corresponding to forward and backward differences, respectively. With the choice
$a=b=0$ no boundary effects are treated, and the derivative is obtained by a 
straightforward piecewise derivative of the polynomial basis. This inevitably leads
to a reduction of the approximation order as the basis is effectively reduced from
order $k$ to order $k-1$. The way to maintain the high approximation order is then
to use the boundary information ($a=b=1/2$) whenever the approximation order of
the finite differences ($O((2^{-n})^2)$) is at least as high as the approximation of the 
basis ($O(2^{-n(k-1)})$). This is important to consider in adaptive function representations,
where parts of the function can be represented at relatively coarse scales, where
the inclusion of the boundary term can severly limit the approximation order of
the differentiated function.

\subsection{Multiresolution operators in $d$ dimensions}
We assume that we have a separable representation of a $d$-dimensional
operator $\mathcal{T}$ such that
\begin{equation}
    \mathcal{T} = \bigotimes_{i=1}^d T_i
\end{equation}
where $T_p$ correspond to a one-dimensional operator as described above.
As for the one-dimensional case we have the equation
\begin{equation}
    \scalingrep{\hat{g}}{n+1} = \bigotimes^d\ \T{n+1}{n+1} \scalingrep{f}{n+1}
\end{equation}
which we can decompose to
\begin{equation}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\bigotimes^d\ \Big(\ \A{n}{n} +\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) 
	\big(\scalingrep{f}{n} + \waveletrep{f}{n}\big)
\end{equation}
and we can simplify the notation in the following way
\begin{equation}
    \A{n}{n} = O^{11,n} \qquad
    \B{n}{n} = O^{10,n} \qquad
    \C{n}{n} = O^{01,n} \qquad
    \T{n}{n} = O^{00,n}
\end{equation}
and the tensor product of the operator can be written
\begin{equation}
    \bigotimes^d\ \Big(\ \A{n}{n}+\ \B{n}{n} +\ \C{n}{n}+\ \T{n}{n}\Big) =
	\sum_{\alpha=0}^{2^d-1} \sum_{\beta=0}^{2^d-1} O^{\alpha,\beta,n}
\end{equation}
where we define
\begin{equation}
    O^{\alpha\beta,n} \mydef \bigotimes_{i}^d\ O^{\alpha_i\beta_i,n}
\end{equation}
with $0 \leq \alpha < 2^d$ and $0 \leq \beta < 2^d$ and $\alpha_i$ and $\beta_i$
are defined by the binary expansion of $\alpha$ and $\beta$ in $d$ dimensions.
We can now obtain a completely equivalent structure as for the mono-dimensional
case
\begin{equation}
    \scalingrep{g}{n} + \waveletrep{g}{n} = 
	\Big(\mathcal{A}^n + \mathcal{B}^n + \mathcal{C}^n + \mathcal{T}^n\Big)
	\big(\scalingrep{f}{n} + \waveletrep{n}{n}\big)
\end{equation} 
with the following definitions
\begin{equation}
    \mathcal{A}^{n} \mydef 
    \sum_{\alpha=1}^{2^d-1} \sum_{\beta=1}^{2^d-1} O^{\alpha\beta,n}, \quad
    \mathcal{B}^{n} \mydef 
    \sum_{\alpha=1}^{2^d-1} O^{\alpha 0,n}, \quad
    \mathcal{C}^{n} \mydef 
    \sum_{\beta=1}^{2^d-1} O^{0 \beta,n}, \quad 
    \mathcal{T}^{n} \mydef O^{00,n}
\end{equation}
We could now proceed with a further decomposition of the scaling parts of the
operator and functions to the next coarser scale, obtaining the standard
representation of the operator in multiple dimension. However, the notation
then becomes very complicated, and, as we will see in the later in the 
implementation part in section \ref{sec:implementation}, this is not needed
in the so-called non-standard representation of the operator.

\section{Separated representation of operators}
In the discussion of multi-dimensional operators in the previous section it
was assumed that we have an operator that is separable in the Cartesian
coordinates. This assumption is necessary in order to make calculations
feasable in higher dimensions, as the straightforward generalization of a
one-dimensional approach leads to a prohibitive exponential scaling in the 
dimension. It is,
however, not necessary that the operator separates exactly, and Beylkin 
\etal~\cite{Beylkin} shows that the integral kernel of many physically 
interresting operators can be approximated as a linear combination of 
products of one-dimensional kernels
\begin{equation}
    \label{eq:sep_rep}
    K(\bs{x},\bs{y}) \approx \hat{K}(\bs{x},\bs{y}) \mydef 
	\sum_{\kappa=1}^{M}\alpha_\kappa \prod_{p=1}^d K_p^\kappa(x_p,y_p)
\end{equation}
The accuracy of this separated representation can be controlled by adapting
the functions $K_p^\kappa$, the expansion coefficients $\alpha_\kappa$ and the 
separation rank $M$, and any precision can in principle be achieved. Such
a representation allows the multi-dimensional operator to be applied one 
dimension at the time, reducing the computational complexity from
$k^{2d}$ per grid cell of the full non-separable operator, to $Mdk^{d+1}$ 
per grid cell of the separated representation in Eq.~(\ref{eq:sep_rep}),
where $k$ is the order of the polynomial basis. While the scaling is still 
exponential in the dimension, the exponent is sufficiently reduced for the 
approach to be applicable for $d=2,3$. 

\subsection{Poisson kernel}
The Poisson equation is usually written in its differential form
\begin{equation}
    \nabla^2 g(\bs{x}) = -f(\bs{x})
\end{equation}
and the solution of can be expressed in terms of the convolution integral
\begin{equation}
    g(\bs{x}) = \int P(\bs{x}-\bs{y})f(\bs{y})\ud\bs{y}
\end{equation}
where $P(\bs{x}-\bs{y})$ is the Green's function satisfying the fundamental 
equation with free boundary conditions (zero at infinity)
\begin{equation}
    \label{eq:fundamental_poisson}
    \nabla^2 P(\bs{x}-\bs{y}) = -\delta(\bs{x}-\bs{y})
\end{equation}
This equation can be solved analytically and the Green's function for
the Poisson equation for $d=3$ is given as
\begin{equation}
    \label{eq:poisson_kern}
    P(\bs{x}-\bs{y}) = \frac{1}{4\pi||\bs{x}-\bs{y}||}
\end{equation}

\subsection{Helmholtz kernel}
The inhomogeneous Helmholtz equation is a generalization of the Poisson equation,
and is given in differential form
\begin{equation}
    \big(\nabla^2 + \mu^2\big) g(\bs{x}) = -f(\bs{x})
\end{equation}
The solution can again be expressed as an integral
\begin{equation}
    g(\bs{x}) = \int H^\mu(\bs{x}-\bs{y})f(\bs{y})\ud\bs{y}
\end{equation}
using the Helmholtz kernel $H^\mu(\bs{x}-\bs{y})$, which is the Greeen's function 
satisfying the fundamental equation
\begin{equation}
    \label{eq:fundamental_helmholtz}
    \big(\nabla^2 + \mu^2\big) H^\mu(\bs{x}-\bs{y}) = -\delta(\bs{x}-\bs{y})
\end{equation}
with zero boundary conditions at infinity. The Green's function for the Helmholtz 
equation for $d=3$ is known analytically as
\begin{equation}
    \label{eq:helmholtz_kern}
    H^\mu(\bs{x}-\bs{y}) = \frac{e^{-\mu\|\bs{x}-\bs{y}\|}}{4\pi||\bs{x}-\bs{y}||}
\end{equation}

\subsection{Separation using Gaussians}
Neither the Poisson nor Helmholtz kernels are separable in the Cartesian coordinates, 
but it is possible to obtain a separated representation as in Eq.~(\ref{eq:sep_rep}) 
of low rank using Gaussian functions
\begin{equation}
    \label{eq:gaussian_exp}
    K(\bs{x}-\bs{y}) \approx \hat{K}(\bs{x}-\bs{y}) = 
	\sum_{\kappa=1}^M \alpha_\kappa e^{-\beta_\kappa \|\bs{x}-\bs{y}\|^2}
\end{equation}
This representation is motivated by the well known integral representation of the 
Poisson kernel\cite{something}
\begin{equation}
    \label{eq:poisson_int_rep}
    P(r) = \frac{1}{r} = \frac{4}{\sqrt{\pi}}\int_0^\infty e^{-4r^2t^2}\ud t, 
	\quad r \mydef \|\bs{x}-\bs{y}\|
\end{equation}
and the parameters in Eq.~(\ref{eq:gaussian_exp}) are obtained in the case of the Poisson 
kernel by transforming Eq.~(\ref{eq:poisson_int_rep}) into an integral of super-exponential 
decay, and discretizing using the trapezoidal rule\cite{Harrison,Frediani}. In this way, 
and similarly in the case of the Helmholtz kernel, it is possible to obtain a separated 
representation $\hat{K}$ of Eq.(\ref{eq:gaussian_exp}) with accuracy $\epsilon_s$ over a 
finite interval
\begin{equation}
    sup_{r>0}\Big|\frac{K(r)-\hat{K}(r)}{K(r)}\Big| < \epsilon_s, \quad r\in[r_0,r_1]
\end{equation}
where the upper bound $r_1$ should be chosen as the longest possible distance
in the computational domain ($r_1=\sqrt{3}$ for the unit cube), and the lower
bound $r_0$ should be chosen so that the contribution due to the integration
at the singularity can be neglected\cite{Frediani}.

\subsection{Derivative kernel}
As a final note we show how we can obtain approximate representations of the derivative 
operator using the same framework as presented above. The derivative operator can be
expressed as
\begin{equation}
    \frac{\ud}{\ud x} f(x) = \int \frac{\ud}{\ud x} \delta(x-y) f(y) \ud y
\end{equation}
where the delta function can be approximated by a high-exponent Gaussian
\begin{equation}
    \label{eq:delta_approx}
    \delta(x-y) \approx \sqrt{\frac{\pi}{\beta}} e^{-\beta (x-y)^2}
\end{equation}
which is normalized so that it integrates to unity. This approximation can be 
differentiated, and the derivative operator can be expressed as the integral
\begin{equation}
    \frac{\ud}{\ud x} f(x) = \int D(x-y)f(y) \ud y
\end{equation}
using the derivative kernel
\begin{equation}
    D(x-y)  = \frac{\ud}{\ud x} \sqrt{\frac{\pi}{\beta}}e^{-\beta (x-y)^2} 
	    = -2 \sqrt{\beta\pi}(x-y)e^{-\beta (x-y)^2}
\end{equation}
This representation approaches the exact derivative as defined by Alpert 
\etal~\cite{Alpert} and presented in section \ref{sec:diff_oper} as the Gaussian 
in Eq.~(\ref{eq:delta_approx}) approaches the delta function ($\beta \rightarrow \infty$).
