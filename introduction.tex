\chapter{Introduction}
\section{Numerical analysis and real-world physics}
The aim of the natural sciences is to model the complex processes occuring in
nature as accurately as possible. It is a remarkable fact that the fundamental 
features of nature are so well described in terms of mathematics, by simple 
and elegant expressions like the wave equation, Newton's laws of motion and 
gravitation, and Maxwell's equations of electrodynamics. Equally remarkable 
is it that these simple expressions can give rise to the vast complexity that 
we observe in the world around us.

The underlying complexity of the these equations means that analytic solutions 
are available only for very simple, idealized systems, often with high symmetry, 
thus limiting their practial usefullness. Over the years, not few science students 
have been questioning the applicability of computing a cannon ball's 
trajecory in vacuum or the electric field around a point charge alone in the universe.

%The true power of physics, of course, lies in its generality. The bird flying 
%above your head, the turbulent water fall, the waves on the ocean, even the 
%movement of the moon and the sun across the sky, are all governed by the same 
%fundamental laws of physics. It is not difficult to imagine each and every water 
%droplet as a tiny cannon ball floating through space, only slightly influenced by 
%all its fellow water droplets, together making up the turbulent, unpredictable 
%motion of a water fall. You can understand \emph{why} nature behaves as it does, 
%even if the process is way to complex for a solution of the underlying equations 
%to available by paper and pencil.

The bridge between the idealized model systems and what we observe in the real 
world is made through numerical analysis, which involves the translation of the 
physical equations into the language of the digital computer. Most modern applied 
sciences relies heavily upon numerical analysis and simulations, either for 
performing numerically intensive calculations or for analysing large amounts of 
data. Over the last decades computer simulation has emerged as a third way in 
science besides the experimental and theoretical approach, and has become an 
indispesable tool for the investigation and prediction of physical and chemical 
processes.

However, with the breakdown of Moore's law (on a single computational device) the 
computational scientist cannot blindly rely on the advances of computer technology 
in order to push the limits of the attainable accuracy and the size of the systems, 
and a lot more responsibility is put back to the computational scientist in developing 
algorithms suitable for parallel execution. While the computational speed of a 
processor no longer can be said to double every second year, Moore's law continues 
to be valid in a more fundamental sense, as the number of transistors continues to 
grow, but in the form of multi-core processors. This means this in the future we 
might see a paradigm change where currently inferior numerical methods and algorithms 
will enter the stage because of favourable scaling with respect to system size 
\emph{and} with the number of processors.

\section{Chemistry without chemicals}
Scientists have for centuries sought an \emph{ab initio} theory of chemical phenomena, 
where molecular structure, properties and reactions can be computed with a minimal amount 
of empirical parameters, but without the fundamental knowledge of the building blocks of 
matter this was for a long time a hopeless endeavor. With the introduction of quantum 
mechanics almost a century ago, the complete physical theory for molecular systems became 
available, but although the exact problem is decievingly simple to state for an arbitrary 
system through the Schr\"{o}dinger equation
\begin{equation}
    \hat{H}\Wavefunction = E\Wavefunction
\end{equation}
its solution for many-body problems is quite the opposite. In fact, whenever the system 
contains more than two particles the problem \emph{cannot} be solved (at least not in 
the usual sense in terms of the standard elementary functions of calculus).

The most common approach in modern computational chemistry is the self-consistent field 
approximations that are based on the familiar chemical concept of one-electron orbitals
$\varphi_i$, each a solution of a Schr\"{o}dinger-like equation
\begin{equation}
    \hat{F}\varphi_i = \epsilon_i\varphi_i
\end{equation}
While the solution of this set of $N$ coupled, nonlinear, three-dimensional partial
differential equations is still a formidable computational task, the complexity of the 
full $3N$-dimensional Schr\"{o}dinger equation has been sufficiently reduced for the 
numerical solution to be feasable for systems with a remarkable number of particles.

This has been made possible by combining a great deal of chemical intuition into the 
development of computational methods. In particular, the introduction of the atomic 
orbital basis in the form of atom-centered Gaussians can be attributed most of the 
success of modern computational chemistry, by providing efficient and compact 
representations with a consistent cancellation of errors. 

However, although the Gaussian basis is ideal for obtaining qualitative numbers fast, 
it struggles when high precision is required. Moreover, as the Gaussian functions 
extend throughout the entire system, it is difficult to reduce the problem into truly 
independent tasks that can be easily distributed among several computers and executed 
simultaneously.

The alternative to the elegant, compact representations using a carefully chosen, 
preoptimized atomic orbital basis, would be a brute force numerical solution using 
real-space representations in terms of numerical grids or finite elements. Such an 
approach would yield robust, unbiased results that do not rely on cancellation of 
errors (but neither would it benefit from it).

It is a well-known fact that the electronic density in molecular systems is rapidly 
varying in the vicinity of the atomic nuclei, and a usual problem with real-space 
methods is that an accurate treatment of the system requires high resolution of grid 
points in the nuclear regions. Keeping this high resolution uniformly througout the 
computaitonal domain would yield unnecessary high accuracy in the interatomic regions, 
thus the real-space treatment of molecular systems is demanding a \emph{multiresolution} 
framework in order to achieve numerical efficiency.

\section{Multiwavelets}
As the theory of wavelets is vast and can be considered a rather advanced topic
of applied mathematics, it remains unfamiliar to most chemists. However, 
Alpert's\cite{Alpert:1993p5460} construction of \emph{multiwavelets} is rather 
simple. Starting with a small set of polynomials $\lbrace\scaling_j\rbrace_{j=0}^k$ 
of order $\leq k$ on the unit interval, we attempt to represent a given function. 
If this basis turns out to be too crude to accurately describe the function, we 
can increase the flexibility by adding higher order polynomials (thus increasing 
the polynomial order $k$), and we approach a complete basis (and an exact 
representation) as $k\rightarrow\infty$.

Alpert shows that there is a second way to approach completeness in this basis.
Instead of increasing the polynomial order, we split the interval and double 
the number of basis functions by dilating and translating the original basis 
to both subintervals
\begin{equation}
    \scaling_{j,l}^1(x) = 2^{1/2}\scaling_j(2x - l), \qquad l = 0,1
\end{equation}
The splitting procedure can be continued until we have reached a scale $n$ where 
we are satisfied with the accuracy of the representation. At this level of 
refinement the unit interval has been split into $2^n$ intervals, each of size 
$2^{-n}$ containing a dilated and translated version of the original $k$-order basis
\begin{equation}
    \scaling_{j,l}^n(x) = 2^{n/2}\scaling_j(2^nx - l), \qquad l = 0,\dots,2^n-1
\end{equation}
This basis can be used to represent any square integrable function to any finite 
accuracy by adjusting the polynomial order $k$ and/or the level of refinement $n$. 
The construction in three dimensions is similar, where at refinement level $n$
the unit cube has been uniformly divided into $2^{3n}$ subcubes. 

The main advantage of multiwavelets over the similar finite element bases is the
possibility of constructing non-uniform grids, and thus focusing the computational
efforts into the problematic nuclear region. Moreover, the grid construction can 
be completely automated to yield representations with guaranteed accuracy.

Although similar constructions were already familiar through the multigrid approaches 
within the finite element community, these methods suffered from a lack of mathematical
rigour and generality, with complicated problem-specific algorithms. Alpert's construction, 
on the other hand, was founded upon the well established, powerful theory of wavelets, 
making the basis applicable to a wide variety of physical problems and operators,
yielding sparse representations and fast algorithms.

\section{Organization of the thesis}
The multiwavelet basis is described in detail within the framework of multiresolution 
analysis in Chap.~\ref{chap:mathematics}, and the practical implementation of this 
formalism into a working computer code is presented in Chap.~\ref{chap:implementation}. 
In particular, we describe the mathematical operations necessary in order to solve the 
equations appearing in the self-consistent field methods of quantum chemistry. An 
introduction to these methods is given in Chap.~\ref{chap:chemistry}, together with 
algorithms for their numerical solution. Finally, in Chap.~\ref{chap:of-dft}, a brief 
discussion is given on the orbital-free methods of density functional theory, and
some preliminary results are presented.

Included in this thesis are also three papers submitted for publication, that can be
considered linked to each of the three main chapters. The first paper involves the
construction of the multiwavelet basis and is an attempt to reduce the memory 
requirements of the method by decreasing the polynomial order $k$ of the basis as 
the level of refinement $n$ is increased. 

The second paper describes the parallel implementation of the code with particular 
focus on the calculation of electrostatic potentials. The performance of the code 
(numerical accuracy, linear scaling of computational time with respect to system 
size, and parallel efficiency) is demonstrated on realistic molecular systems of 
up to 600 atoms.

The topic of the third paper is the solution of the self-consistent field problem
in quantum chemistry. General algorithms are presented for the iterative solution
of the Hartree-Fock and Kohn-Sham equations for many-electron systems in both a
canonical and localized orbital framework. High accuracy energies are presented 
for small molecules, while robust and fast convergence is demonstrated for small 
and medium sized systems (less than 100 electrons).


